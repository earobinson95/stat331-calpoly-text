[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 331/531 Statistical Computing with R",
    "section": "",
    "text": "Preface\nThis text has been modified from material by Dr. Susan VanderPlas – see Statistical Computing using R and Python for her course book – with integration of content and videos from Dr. Allison Theobold and Dr. Kelly Bodwin.\nThis text is designed to demonstrate statistical programming concepts and techniques in R. It is intended as a substitute for hours and hours of video lectures - watching someone code and talk about code is not usually the best way to learn how to code. It’s far better to learn how to code by … coding.\nI hope that you will work through this text week by week over the quarter. I have included comics, snark, gifs, YouTube videos, extra resources, and more: my goal is to make this a collection of the best information I can find on statistical programming.\nIn most cases, this text includes way more information than you need. Everyone comes into this class with a different level of computing experience, so I’ve attempted to make this text comprehensive. Unfortunately, that means some people will be bored and some will be overwhelmed. Use this text in the way that works best for you - skip over the stuff you know already, ignore the stuff that seems too complex until you understand the basics. Come back to the scary stuff later and see if it makes more sense to you."
  },
  {
    "objectID": "index.html#how-to-use-this-text",
    "href": "index.html#how-to-use-this-text",
    "title": "Stat 331/531 Statistical Computing with R",
    "section": "How to Use This Text",
    "text": "How to Use This Text\nI’ve made an effort to use some specific formatting and enable certain features that make this text a useful tool for this class.\nSpecial Sections\n\nCheck-in\nCheck-in sections contain quizzes or preview activities you must complete and submit to Canvas for credit.\n\nTry It Out\nTry it out sections contain the required weekly practice activities (and sometimes additional/optional activities) you should do to reinforce the things you’ve just read.\n\nWatch Out\nWatch out sections contain things you may want to look out for - common errors, etc.\n\nExamples\nExample sections contain code and other information. Don’t skip them!\n\nGo Read\nSometimes, there are better resources out there than something I could write myself. When you see this section, go read the enclosed link as if it were part of the book.\n\nLearn More\nLearn More sections contain other references that may be useful on a specific topic. Suggestions are welcome (email me to suggest a new reference that I should add), as there’s no way for one person to catalog all of the helpful programming resources on the internet!\n\nMy Opinion\nThese sections contain things you should definitely not consider as fact and should just take with a grain of salt.\n\nNote\nNote sections contain clarification points (anywhere I would normally say “note that ….)\n\nExpandable Sections\n\nThese are expandable sections, with additional information when you click on the line\nThis additional information may be information that is helpful but not essential, or it may be that an example just takes a LOT of space and I want to make sure you can skim the book without having to scroll through a ton of output.\n\nMany times, examples will be in expandable sections\nThis keeps the code and output from obscuring the actual information in the textbook that I want you to retain. You can always look up the syntax, but you do need to absorb the details I’ve written out."
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Stat 331/531 Statistical Computing with R",
    "section": "Additional Resources",
    "text": "Additional Resources\nReferences or additional readings may come from the following texts:\n\nR for Data Science (2e)\nAdvanced R\nModern Dive\nStat 545 (Data wrangling, exploration, and analysis with R) by Jenny Bryan\n\nYou can find additional help for Coding in R from the following resources:\n\nRStudio Education Page\nPosit Primers\n“R Bootcamp” Practice\n\nPart of this course will be dedicated to conducting familiar statistical analyses using R code. You can find a refresher on statistical concepts from the following resources:\n\nIntroduction to Modern Statistics\nModern Dive"
  },
  {
    "objectID": "00-prereading.html#ch0-objectives",
    "href": "00-prereading.html#ch0-objectives",
    "title": "Pre-reading",
    "section": "Objectives",
    "text": "Objectives\nA prerequisite for this course is an introductory programming course. Therefore, there is some assumed knowledge. Maybe you have not seen these concepts in R, but you have already developed a base for logically thinking through a computing problem in another language.\nThis chapter is meant to provide a resource for the basics of programming (in R) and gives me a place to refer back to (as need be) in future chapters.\nIn this chapter you will:\n\nLearn the basics of a computer system.\nRefresh your mathematical logic and apply it to variables, vectors, and matrices.\nKnow the different types of variables and how to assign them to objects in R.\nUnderstand how to create and index vectors and matrices in R.\nExtend your logic to control structures with if-then statements and loops."
  },
  {
    "objectID": "00-prereading.html#computer-basics",
    "href": "00-prereading.html#computer-basics",
    "title": "Pre-reading",
    "section": "Computer Basics",
    "text": "Computer Basics\nIt is helpful when teaching a topic as technical as programming to ensure that everyone starts from the same basic foundational understanding and mental model of how things work. When teaching geology, for instance, the instructor should probably make sure that everyone understands that the earth is a round ball and not a flat plate – it will save everyone some time later.\nWe all use computers daily - we carry them around with us on our wrists, in our pockets, and in our backpacks. This is no guarantee, however, that we understand how they work or what makes them go.\nHardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). Most of the other details aren’t necessary (for now).\nOperating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time.\n\n\n\nFile Systems\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore. This article is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\nStop watching at 4:16.\nThat’s not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\n\nRecommend watching - helpful for understanding file paths!\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders).\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your “base folder” is known as your working directory - the place your program thinks of as home.\nIn Chapter 1, we will discuss Directories, Paths, and Projects as they relate to R and getting setup to be successful in this course."
  },
  {
    "objectID": "00-prereading.html#vectors-matrices-and-arrays",
    "href": "00-prereading.html#vectors-matrices-and-arrays",
    "title": "Pre-reading",
    "section": "Vectors, Matrices, and Arrays",
    "text": "Vectors, Matrices, and Arrays\n\n\nThis section introduces some of the most important tools for working with data: vectors, matrices, loops, and if statements. It would be nice to gradually introduce each one of these topics separately, but they tend to go together, especially when you’re talking about programming in the context of data processing.\nMathematical Logic\nBefore we start talking about data structures and control structures, though, we’re going to take a minute to review some concepts from mathematical logic. This will be useful for both data structures and control structures, so stick with me for a few minutes.\nAnd, Or, and Not\nWe can combine logical statements using and, or, and not.\n\n(X AND Y) requires that both X and Y are true.\n(X OR Y) requires that one of X or Y is true.\n(NOT X) is true if X is false, and false if X is true. Sometimes called negation.\n\nIn R, we use ! to symbolize NOT.\nOrder of operations dictates that NOT is applied before other operations. So NOT X AND Y is read as (NOT X) AND (Y). You must use parentheses to change the way this is interpreted.\n\nx &lt;- c(TRUE, FALSE, TRUE, FALSE)\ny &lt;- c(TRUE, TRUE, FALSE, FALSE)\n\nx & y # AND\n\n[1]  TRUE FALSE FALSE FALSE\n\nx | y # OR\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n!x & y # NOT X AND Y\n\n[1] FALSE  TRUE FALSE FALSE\n\nx & !y # X AND NOT Y\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nDe Morgan’s Laws\nDe Morgan’s Laws are a set of rules for how to combine logical statements. You can represent them in a number of ways:\n\nNOT(A or B) is equivalent to NOT(A) and NOT(B)\nNOT(A and B) is equivalent to NOT(A) or NOT(B)\n\n\n\nDefinitions\nDeMorgan’s First Law\nDeMorgan’s Second Law\n\n\n\n Suppose that we set the convention that .\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A OR B (aka NOT (A OR B)) is the same as the region that is outside of (NOT A) and (NOT B)\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A AND B (aka NOT (A AND B)) is the same as the region that is outside of (NOT A) OR (NOT B)\n\n\n\n\nBasic Data Types\nWhile we will discuss data types more in depth during class, it is important to have a base grasp on the types of data you might see in a programming language.\nValues and Types\nLet’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn R, there are some very basic data types:\n\nlogical or boolean - FALSE/TRUE or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\n\ndouble or float or numeric- decimal numbers.\n\n\nfloat is short for floating-point value.\n\ndouble is a floating-point value with more precision (“double precision”).1\n\n\nR uses the name numeric to indicate a decimal value, regardless of precision.\n\n\ncharacter or string - holds text, usually enclosed in quotes.\n\nIf you don’t know what type a value is, R has a function to help you with that.\n\nclass(FALSE)\nclass(2L) # by default, R treats all numbers as numeric/decimal values. \n          # The L indicates that we're talking about an integer. \nclass(2)\nclass(\"Hello, programmer!\")\n\n[1] \"logical\"\n[1] \"integer\"\n[1] \"numeric\"\n[1] \"character\"\n\n\n\nIn R, boolean values are TRUE and FALSE. Capitalization matters a LOT.\nOther things matter too: if we try to write a million, we would write it 1000000 instead of 1,000,000. Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data.\n\nVariables\nProgramming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\nIn R, we assign variables values using the syntax object_name &lt;- value You can read this as “object name gets value” in your head.\n\nmessage &lt;- \"So long and thanks for all the fish\"\nyear &lt;- 2025\nthe_answer &lt;- 42L\nearth_demolished &lt;- FALSE\n\n\nNote that in R, we assign variables values using the &lt;- operator. Technically, = will work for assignment, but &lt;- is more common than = in R by convention.\n\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\nValid Names\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n– Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, _, and . in R.\nWhat happens if we try to create a variable name that isn’t valid?\nStarting a variable name with a number will get you an error message that lets you know that something isn’t right - “unexpected symbol”.\n\n1st_thing &lt;- \"check your variable names!\"\n\nError: &lt;text&gt;:1:2: unexpected symbol\n1: 1st_thing\n     ^\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\nWhy is naming things hard? - Blog post by Neil Kakkar\n\nThere are a few different conventions for naming things that may be useful:\n\n\nsome_people_use_snake_case, where words are separated by underscores\n\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\nsome.people.use.periods\nA few people mix conventions with variables_thatLookLike.this and they are almost universally hated.\n\nAs long as you pick ONE naming convention and don’t mix-and-match, you’ll be fine. It will be easier to remember what you named your variables (or at least guess) and you’ll have fewer moments where you have to go scrolling through your script file looking for a variable you named.\nType Conversions\nWe talked about values and types above, but skipped over a few details because we didn’t know enough about variables. It’s now time to come back to those details.\nWhat happens when we have an integer and a numeric type and we add them together? Hopefully, you don’t have to think too hard about what the result of 2 + 3.5 is, but this is a bit more complicated for a computer for two reasons: storage, and arithmetic.\nIn days of yore, programmers had to deal with memory allocation - when declaring a variable, the programmer had to explicitly define what type the variable was. This tended to look something like the code chunk below:\nint a = 1\ndouble b = 3.14159\nTypically, an integer would take up 32 bits of memory, and a double would take up 64 bits, so doubles used 2x the memory that integers did. R is dynamically typed, which means you don’t have to deal with any of the trouble of declaring what your variables will hold - the computer automatically figures out how much memory to use when you run the code. So we can avoid the discussion of memory allocation and types because we’re using higher-level languages that handle that stuff for us2.\nBut the discussion of types isn’t something we can completely avoid, because we still have to figure out what to do when we do operations on things of two different types - even if memory isn’t a concern, we still have to figure out the arithmetic question.\nSo let’s see what happens with a couple of examples, just to get a feel for type conversion (aka type casting or type coercion), which is the process of changing an expression from one data type to another.\n\nmode(2L + 3.14159) # add integer 2 and pi\n\n[1] \"numeric\"\n\nmode(2L + TRUE) # add integer 2 and TRUE\n\n[1] \"numeric\"\n\nmode(TRUE + FALSE) # add TRUE and FALSE\n\n[1] \"numeric\"\n\n\nAll of the examples above are ‘numeric’ - basically, a catch-all class for things that are in some way, shape, or form numbers. Integers and decimal numbers are both numeric, but so are logicals (because they can be represented as 0 or 1).\nYou may be asking yourself at this point why this matters, and that’s a decent question. We will eventually be reading in data from spreadsheets and other similar tabular data, and types become very important at that point, because we’ll have to know how R handles type conversions.\nTest it out!\nDo a bit of experimentation - what happens when you try to add a string and a number? Which types are automatically converted to other types? Fill in the following table in your notes:\nAdding a ___ and a ___ produces a ___:\n\n\nLogical\nInteger\nDecimal\nString\n\n\n\n\nLogical\n\n\n\n\n\n\nInteger\n\n\n\n\n\n\nDecimal\n\n\n\n\n\n\nString\n\n\n\n\n\n\nAbove, we looked at automatic type conversions, but in many cases, we also may want to convert variables manually, specifying exactly what type we’d like them to be. A common application for this in data analysis is when there are “*” or “.” or other indicators in an otherwise numeric column of a spreadsheet that indicate missing data: when this data is read in, the whole column is usually read in as character data. So we need to know how to tell R that we want our string to be treated as a number, or vice-versa.\nIn R, we can explicitly convert a variable’s type using as.XXX() functions, where XXX is the type you want to convert to (as.numeric, as.integer, as.logical, as.character, etc.).\n\nx &lt;- 3\ny &lt;- \"3.14159\"\n\nx + y\n\nError in x + y: non-numeric argument to binary operator\n\nx + as.numeric(y)\n\n[1] 6.14159\n\n\nOperators and Functions\nIn addition to variables, functions are extremely important in programming.\nLet’s first start with a special class of functions called operators. You’re probably familiar with operators as in arithmetic expressions: +, -, /, *, and so on.\nHere are a few of the most important ones:\n\n\nOperation\nR symbol\n\n\n\nAddition\n+\n\n\nSubtraction\n-\n\n\nMultiplication\n*\n\n\nDivision\n/\n\n\nInteger Division\n%/%\n\n\nModular Division\n%%\n\n\nExponentiation\n^\n\n\n\nNote that integer division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\nSo 14 %/% 3 would be 4, and 14 %% 3 would be 2.\n\n14 %/% 3\n\n[1] 4\n\n14 %% 3\n\n[1] 2\n\n\nNote that these operands are all intended for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated.\nOrder of Operations\nR operates under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\n(1+1)^(5-2) # 2 ^ 3 = 8\n\n[1] 8\n\n1 + 2^3 * 4 # 1 + (8 * 4)\n\n[1] 33\n\n3*1^3 # 3 * 1\n\n[1] 3\n\n\nString Operations\nYou will have to use functions to perform operations on strings, as R does not have string operators. In R, to concatenate things, we need to use functions: paste or paste0:\n\npaste(\"first\", \"second\", sep = \" \")\n\n[1] \"first second\"\n\npaste(\"first\", \"second\", collapse = \" \")\n\n[1] \"first second\"\n\npaste(c(\"first\", \"second\"), sep = \" \") # sep only works on separate parameters\n\n[1] \"first\"  \"second\"\n\npaste(c(\"first\", \"second\"), collapse = \" \") # collapse works on vectors\n\n[1] \"first second\"\n\npaste(c(\"a\", \"b\", \"c\", \"d\"), \n      c(\"first\", \"second\", \"third\", \"fourth\"), \n      sep = \"-\", collapse = \" \")\n\n[1] \"a-first b-second c-third d-fourth\"\n\n# sep is used to collapse parameters, then collapse is used to collapse vectors\n\npaste0(c(\"a\", \"b\", \"c\"))\n\n[1] \"a\" \"b\" \"c\"\n\npaste0(\"a\", \"b\", \"c\") # equivalent to paste(..., sep = \"\")\n\n[1] \"abc\"\n\n\nYou don’t need to understand the details of this at this point in the class, but it is useful to know how to combine strings.\nFunctions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, operators (like those above) are a special type of functions – but we aren’t going to get into that now.\nWe’re also not going to talk about how to create our own functions just yet. Instead, I’m going to show you how to use functions.\nIt may be helpful at this point to print out the R reference card3. This cheat sheet contains useful functions for a variety of tasks.\nMethods are a special type of function that operate on a specific variable type. In R, you would get the length of a string variable using length(my_string).\nRight now, it is not really necessary to know too much more about functions than this: you can invoke a function by passing in arguments, and the function will do a task and return the value.\nData Structures\nIn the previous section, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complicated arrangements of information.\n\n\nHomogeneous\nHeterogeneous\n\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\nLists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\nA lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\nx &lt;- list(\"a\", 3, FALSE)\nx\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] FALSE\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. We call that process indexing.\nIndexing\nEvery element in a list has an index (a location, indicated by an integer position)4.\nIn R, we count from 1.\n\n\nAn R-indexed lego list, counting from 1 to 5\n\n\nx &lt;- list(\"a\", 3, FALSE)\n\nx[1] # This returns a list\n\n[[1]]\n[1] \"a\"\n\nx[1:2] # This returns multiple elements in the list\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 3\n\nx[[1]] # This returns the item\n\n[1] \"a\"\n\nx[[1:2]] # This doesn't work - you can only use [[]] with a single index\n\nError in x[[1:2]]: subscript out of bounds\n\n\nList indexing with [] will return a list with the specified elements.\nTo actually retrieve the item in the list, use [[]]. The only downside to [[]] is that you can only access one thing at a time.\nWe’ll talk more about indexing as it relates to vectors, but indexing is a general concept that applies to just about any multi-value object.\nVectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want: \n\nIndexing by Location\nEach element in a vector has an index - an integer telling you what the item’s position within the vector is. I’m going to demonstrate indices with the string vector\n\n\n\n\n\n\nR\n\n\n\n\n1-indexed language\n\n\n\nCount elements as 1, 2, 3, 4, …, N\n\n\n\n\n\n\n\n\nIn R, we create vectors with the c() function, which stands for “concatenate” - basically, we stick a bunch of objects into a row.\n\ndigits_pi &lt;- c(3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5)\n\n# Access individual entries\ndigits_pi[1]\n\n[1] 3\n\ndigits_pi[2]\n\n[1] 1\n\ndigits_pi[3]\n\n[1] 4\n\n# R is 1-indexed - a list of 11 things goes from 1 to 11\ndigits_pi[0]\n\nnumeric(0)\n\ndigits_pi[11]\n\n[1] 5\n\n# Print out the vector\ndigits_pi\n\n [1] 3 1 4 1 5 9 2 6 5 3 5\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific things as well:\n\nfavorite_cats &lt;- c(\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\")\n\nfavorite_cats\n\n[1] \"Grumpy\"   \"Garfield\" \"Jorts\"    \"Jean\"    \n\nfavorite_cats[2] &lt;- \"Nyan Cat\"\n\nfavorite_cats\n\n[1] \"Grumpy\"   \"Nyan Cat\" \"Jorts\"    \"Jean\"    \n\n\nIf you’re curious about any of these cats, see the footnotes5.\nIndexing with Logical Vectors\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that goes along with a vector of another type to use as a logical index.\n\n\nlego vectors - a pink/purple hued set of 1x3 bricks representing the data and a corresponding set of 1x1 grey and black bricks representing the logical index vector of the same length\n\nIf we let the black lego represent “True” and the grey lego represent “False”, we can use the logical vector to pull out all values in the main vector.\n\n\n\n\n\n\nBlack = True, Grey = False\nGrey = True, Black = False\n\n\n\n\n\n\nNote that for logical indexing to work properly, the logical index must be the same length as the vector we’re indexing. This constraint will return when we talk about data frames, but for now just keep in mind that logical indexing doesn’t make sense when this constraint isn’t true.\n\n# Define a character vector\nweekdays &lt;- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\nweekend &lt;- c(\"Sunday\", \"Saturday\")\n\n# Create logical vectors\nrelax_days &lt;- c(1, 0, 0, 0, 0, 0, 1) # doing this the manual way\nrelax_days &lt;- weekdays %in% weekend # This creates a logical vector \n                                    # with less manual construction\nrelax_days\n\n[1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nschool_days &lt;- !relax_days # FALSE if weekend, TRUE if not\nschool_days\n\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n\n# Using logical vectors to index the character vector\nweekdays[school_days] # print out all school days\n\n[1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n\n\nReviewing Types\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently-typed things?\n\nc(2L, FALSE, 3.1415, \"animal\") # all converted to strings\n\n[1] \"2\"      \"FALSE\"  \"3.1415\" \"animal\"\n\nc(2L, FALSE, 3.1415) # converted to numerics\n\n[1] 2.0000 0.0000 3.1415\n\nc(2L, FALSE) # converted to integers\n\n[1] 2 0\n\n\nAs a reminder, this is an example of implicit type conversion - R decides what type to use for you, going with the type that doesn’t lose data but takes up as little space as possible.\nMatrices\nA matrix is the next step after a vector - it’s a set of values arranged in a two-dimensional, rectangular format.\nMatrix (Lego)\n\n\nlego depiction of a 3-row, 4-column matrix of 2x2 red-colored blocks\n\n\n# Minimal matrix in R: take a vector, \n# tell R how many rows you want\nmatrix(1:12, nrow = 3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(1:12, ncol = 3) # or columns\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n# by default, R will fill in column-by-column\n# the byrow parameter tells R to go row-by-row\nmatrix(1:12, nrow = 3, byrow = T)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n# We can also easily create square matrices \n# with a specific diagonal (this is useful for modeling)\ndiag(rep(1, times = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\n\nMost of the problems we’re going to work on will not require much in the way of matrix or array operations. For now, you need the following:\n\nKnow that matrices exist and what they are (2-dimensional arrays of numbers)\nUnderstand how they are indexed (because it is extremely similar to data frames that we’ll work with in the next chapter)\nBe aware that there are lots of functions that depend on matrix operations at their core (including linear regression)\nIndexing in Matrices\nR uses [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix, we would use [3,1] to get to the third row and first column entry; in python, we would use [2,0] (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\nmy_mat &lt;- matrix(1:12, nrow = 3, byrow = T)\n\nmy_mat[3,1] &lt;- 500\n\nmy_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]  500   10   11   12\n\n\nMatrix Operations\nThere are a number of matrix operations that we need to know for basic programming purposes:\n\nscalar multiplication \\[c*\\textbf{X} = c * \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] = \\left[\\begin{array}{cc} c*x_{1,1} & c*x_{1, 2}\\\\c*x_{2,1} & c*x_{2,2}\\end{array}\\right]\\]\n\ntranspose - flip the matrix across the left top -&gt; right bottom diagonal. \\[t(\\textbf{X}) = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right]^T = \\left[\\begin{array}{cc} x_{1,1} & x_{2,1}\\\\x_{1,2} & x_{2,2}\\end{array}\\right]\\]\n\nmatrix multiplication (dot product) - you will learn more about this in linear algebra, but here’s a preview. Here is a better explanation of the cross product \\[\\textbf{X}*\\textbf{Y} = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] * \\left[\\begin{array}{cc} y_{1,1} \\\\y_{2,1} \\end{array}\\right] = \\left[\\begin{array}{c}x_{1,1}*y_{1,1} + x_{1,2}*y_{2,1} \\\\x_{2, 1}*y_{1,1} + x_{2,2}*y_{2,1}\\end{array}\\right]\\] Note that matrix multiplication depends on having matrices of compatible dimensions. If you have two matrices of dimension \\((a \\times b)\\) and \\((c \\times d)\\), then \\(b\\) must be equal to \\(c\\) for the multiplication to work, and your result will be \\((a \\times d)\\).\n\n\nx &lt;- matrix(c(1, 2, 3, 4), nrow = 2, byrow = T)\ny &lt;- matrix(c(5, 6), nrow = 2)\n\n# Scalar multiplication\nx * 3\n\n     [,1] [,2]\n[1,]    3    6\n[2,]    9   12\n\n3 * x\n\n     [,1] [,2]\n[1,]    3    6\n[2,]    9   12\n\n# Transpose\nt(x)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nt(y)\n\n     [,1] [,2]\n[1,]    5    6\n\n# matrix multiplication (dot product)\nx %*% y\n\n     [,1]\n[1,]   17\n[2,]   39\n\n\nArrays\nArrays are a generalized n-dimensional version of a vector: all elements have the same type, and they are indexed using square brackets in both R and python: [dim1, dim2, dim3, ...]\nI don’t think you will need to create 3+ dimensional arrays in this class, but if you want to try it out, here is some code.\n\narray(1:8, dim = c(2,2,2))\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\nNote that displaying this requires 2 slices, since it’s hard to display 3D information in a 2D terminal arrangement."
  },
  {
    "objectID": "00-prereading.html#control-structures",
    "href": "00-prereading.html#control-structures",
    "title": "Pre-reading",
    "section": "Control Structures",
    "text": "Control Structures\nThe focus of this course is more on working with data - however in prior programming courses you have likely developed the logical thinking to work with Control structures. Control structures are statements in a program that determine when code is evaluated (and how many times it might be evaluated). There are two main types of control structures: if-statements and loops.\nBefore we start on the types of control structures, let’s get in the right mindset. We’re all used to “if-then” logic, and use it in everyday conversation, but computers require another level of specificity when you’re trying to provide instructions.\n\n\nCheck out this video of the classic “make a peanut butter sandwich instructions challenge”:\n\n\nHere’s another example:\n\n\n‘If you’re done being pedantic, we should get dinner.’ ‘You did it again!’ ‘No, I didn’t.’\n\nThe key takeaways from these bits of media are that you should read this section with a focus on exact precision - state exactly what you mean, and the computer will do what you say. If you instead expect the computer to get what you mean, you’re going to have a bad time.\nConditional Statements\nConditional statements determine if code is evaluated.\nThey look like this:\nif (condition)\n  then\n    (thing to do)\n  else\n    (other thing to do)\nThe else (other thing to do) part may be omitted.\nWhen this statement is read by the computer, the computer checks to see if condition is true or false. If the condition is true, then (thing to do) is also run. If the condition is false, then (other thing to do) is run instead.\n\nLet’s try this out:\n\nx &lt;- 3\ny &lt;- 1\n\nif (x &gt; 2) { \n  y &lt;- 8\n} else {\n  y &lt;- 4\n}\n\nprint(paste(\"x =\", x, \"; y =\", y))\n\n[1] \"x = 3 ; y = 8\"\n\n\nThe logical condition after if must be in parentheses. It is common to then enclose the statement to be run if the condition is true in {} so that it is clear what code matches the if statement. You can technically put the condition on the line after the if (x &gt; 2) line, and everything will still work, but then it gets hard to figure out what to do with the else statement - it technically would also go on the same line, and that gets hard to read.\n\nx &lt;- 3\ny &lt;- 1\n\nif (x &gt; 2) y &lt;- 8 else y &lt;- 4\n\nprint(paste(\"x =\", x, \"; y =\", y))\n\n[1] \"x = 3 ; y = 8\"\n\n\nSo while the 2nd version of the code technically works, the first version with the brackets is much easier to read and understand. Please try to emulate the first version!\nRepresenting Conditional Statements as Diagrams\nA common way to represent conditional logic is to draw a flow chart diagram.\nIn a flow chart, conditional statements are represented as diamonds, and other code is represented as a rectangle. Yes/no or True/False branches are labeled. Typically, after a conditional statement, the program flow returns to a single point.\n\n\nProgram flow diagram outline of a simple if/else statement\n\n\nUS Tax brackets\n\n\nProblem\nSolution\nProgram Flow Chart\n\n\n\nThe US Tax code has brackets, such that the first $10,275 of your income is taxed at 10%, anything between $10,275 and $41,775 is taxed at 12%, and so on.\nHere is the table of tax brackets for single filers in 2022:\n\n\nrate\nIncome\n\n\n\n10%\n$0 to $10,275\n\n\n12%\n$10,275 to $41,775\n\n\n22%\n$41,775 to $89,075\n\n\n24%\n$89,075 to $170,050\n\n\n32%\n$170,050 to $215,950\n\n\n35%\n$215,950 to $539,900\n\n\n37%\n$539,900 or more\n\n\n\nNote: For the purposes of this problem, we’re ignoring the personal exemption and the standard deduction, so we’re already simplifying the tax code.\nWrite a set of if statements that assess someone’s income and determine what their overall tax rate is.\nHint: You may want to keep track of how much of the income has already been taxed in a variable and what the total tax accumulation is in another variable.\n\n\n\n# Start with total income\nincome &lt;- 200000\n\n# x will hold income that hasn't been taxed yet\nx &lt;- income\n# y will hold taxes paid\ny &lt;- 0\n\nif (x &lt;= 10275) {\n  y &lt;- x*.1 # tax paid\n  x &lt;- 0 # All money has been taxed\n} else {\n  y &lt;- y + 10275 * .1\n  x &lt;- x - 10275 # Money remaining that hasn't been taxed\n}\n\nif (x &lt;= (41775 - 10275)) {\n  y &lt;- y + x * .12\n  x &lt;- 0\n} else {\n  y &lt;- y + (41775 - 10275) * .12\n  x &lt;- x - (41775 - 10275) \n}\n\nif (x &lt;= (89075 - 41775)) {\n  y &lt;- y + x * .22\n  x &lt;- 0\n} else {\n  y &lt;- y + (89075 - 41775) * .22\n  x &lt;- x - (89075 - 41775)\n}\n\nif (x &lt;= (170050 - 89075)) {\n  y &lt;- y + x * .24\n  x &lt;- 0\n} else {\n  y &lt;- y + (170050 - 89075) * .24\n  x &lt;- x - (170050 - 89075)\n}\n\nif (x &lt;= (215950 - 170050)) {\n  y &lt;- y + x * .32\n  x &lt;- 0\n} else {\n  y &lt;- y + (215950 - 170050) * .32\n  x &lt;- x - (215950 - 170050)\n}\n\nif (x &lt;= (539900 - 215950)) {\n  y &lt;- y + x * .35\n  x &lt;- 0\n} else {\n  y &lt;- y + (539900 - 215950) * .35\n  x &lt;- x - (539900 - 215950)\n}\n\nif (x &gt; 0) {\n  y &lt;- y + x * .37\n}\n\n\nprint(paste(\"Total Tax Rate on $\", income, \" in income = \", round(y/income, 4)*100, \"%\"))\n\n[1] \"Total Tax Rate on $ 2e+05  in income =  22.12 %\"\n\n\n\n\nLet’s explore using program flow maps for a slightly more complicated problem: The tax bracket example that we used to demonstrate if statement syntax.\n\n\nThe control flow diagram for the code in the previous example\n\nControl flow diagrams can be extremely helpful when figuring out how programs work (and where gaps in your logic are when you’re debugging). It can be very helpful to map out your program flow as you’re untangling a problem.\n\n\n\nChaining Conditional Statements: Else-If\nIn many cases, it can be helpful to have a long chain of conditional statements describing a sequence of alternative statements.\n\nAge brackets\nFor instance, suppose I want to determine what categorical age bracket someone falls into based on their numerical age. All of the bins are mutually exclusive - you can’t be in the 25-40 bracket and the 41-55 bracket.\n\nProgram Flow Map\n\n\n\n\nProgram flow map for a series of mutually exclusive categories. If our goal is to take a numeric age variable and create a categorical set of age brackets, such as &lt;18, 18-25, 26-40, 41-55, 56-65, and &gt;65, we can do this with a series of if-else statements chained together. Only one of the bracket assignments is evaluated, so it is important to place the most restrictive condition first.\n\nThe important thing to realize when examining this program flow map is that if age &lt;= 18 is true, then none of the other conditional statements even get evaluated. That is, once a statement is true, none of the other statements matter. Because of this, it is important to place the most restrictive statement first.\n\n\nProgram flow map for a series of mutually exclusive categories, emphasizing that only some statements are evaluated. When age = 40, only (age &lt;= 18), (age &lt;= 25), and (age &lt;= 40) are evaluated conditionally. Of the assignment statements, only bracket = ‘26-40’ is evaluated when age = 40.\n\nIf for some reason you wrote your conditional statements in the wrong order, the wrong label would get assigned:\n\n\nProgram flow map for a series of mutually exclusive categories, with category labels in the wrong order - &lt;40 is evaluated first, and so &lt;= 25 and &lt;= 18 will never be evaluated and the wrong label will be assigned for anything in those categories.\n\nIn code, we would write this statement using else-if (or elif) statements.\n\nage &lt;- 40 # change this as you will to see how the code works\n\nif (age &lt; 18) {\n  bracket &lt;- \"&lt;18\"\n} else if (age &lt;= 25) {\n  bracket &lt;- \"18-25\"\n} else if (age &lt;= 40) {\n  bracket &lt;- \"26-40\"\n} else if (age &lt;= 55) {\n  bracket &lt;- \"41-55\" \n} else if (age &lt;= 65) {\n  bracket &lt;- \"56-65\"\n} else {\n  bracket &lt;- \"&gt;65\"\n}\n\nbracket\n\n[1] \"26-40\"\n\n\n\n\n\nLoops\n\nOften, we write programs which update a variable in a way that the new value of the variable depends on the old value:\nx = x + 1\nThis means that we add one to the current value of x.\nBefore we write a statement like this, we have to initialize the value of x because otherwise, we don’t know what value to add one to.\nx = 0\nx = x + 1\nWe sometimes use the word increment to talk about adding one to the value of x; decrement means subtracting one from the value of x.\nA particularly powerful tool for making these types of repetitive changes in programming is the loop, which executes statements a certain number of times. Loops can be written in several different ways, but all loops allow for executing a block of code a variable number of times.\nWhile Loops\nWe just discussed conditional statements, where a block of code is only executed if a logical statement is true.\nThe simplest type of loop is the while loop, which executes a block of code until a statement is no longer true.\n\n\nFlow map showing while-loop pseudocode (while x &lt;= N) { # code that changes x in some way} and the program flow map expansion where we check if x &gt; N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then change x and start over.\n\n\nx &lt;- 0\n\nwhile (x &lt; 10) { \n  # Everything in here is executed \n  # during each iteration of the loop\n  print(x)\n  x &lt;- x + 1\n}\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\n\n\nWhile loops\n\n\nProblem\nMath Notation\nSolution\n\n\n\nWrite a while loop that verifies that \\[\\lim_{N \\rightarrow \\infty} \\prod_{k=1}^N \\left(1 + \\frac{1}{k^2}\\right) = \\frac{e^\\pi - e^{-\\pi}}{2\\pi}.\\]\nTerminate your loop when you get within 0.0001 of \\(\\frac{e^\\pi - e^{-\\pi}}{2\\pi}\\). At what value of \\(k\\) is this point reached?\n\n\nBreaking down math notation for code:\n\nIf you are unfamiliar with the notation \\(\\prod_{k=1}^N f(k)\\), this is the product of \\(f(k)\\) for \\(k = 1, 2, ..., N\\), \\[f(1)\\cdot f(2)\\cdot ... \\cdot f(N)\\]\nTo evaluate a limit, we just keep increasing \\(N\\) until we get arbitrarily close to the right hand side of the equation.\n\nIn this problem, we can just keep increasing \\(k\\) and keep track of the cumulative product. So we define k=1, prod = 1, and ans before the loop starts. Then, we loop over k, multiplying prod by \\((1 + 1/k^2)\\) and then incrementing \\(k\\) by one each time. At each iteration, we test whether prod is close enough to ans to stop the loop.\n\n\nYou will use pi and exp() - these are available by default without any additional libraries or packages.\n\nk &lt;- 1\nprod &lt;- 1\nans &lt;- (exp(pi) - exp(-pi))/(2*pi)\ndelta &lt;- 0.0001\n\nwhile (abs(prod - ans) &gt;= 0.0001) {\n  prod &lt;- prod * (1 + 1/k^2)\n  k &lt;- k + 1\n}\n\nk\n\n[1] 36761\n\nprod\n\n[1] 3.675978\n\nans\n\n[1] 3.676078\n\n\n\n\n\nFor Loops\nAnother common type of loop is a for loop. In a for loop, we run the block of code, iterating through a series of values (commonly, one to N, but not always). Generally speaking, for loops are known as definite loops because the code inside a for loop is executed a specific number of times. While loops are known as indefinite loops because the code within a while loop is evaluated until the condition is falsified, which is not always a known number of times.\n\n\nFlow Map\nR\n\n\n\n\n\nFlow map showing for-loop pseudocode (for j in 1 to N) { # code} and the program flow map expansion where j starts at 1 and we check if j &gt; N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then increment j and start over.\n\n\n\n\nfor (i in 1:5 ) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nFor loops are often run from 1 to N but in essence, a for loop is run for every value of a vector (which is why loops are included in the same chapter as vectors).\n\nFor instance, in R, there is a built-in variable called month.name. Type month.name into your R console to see what it looks like. If we want to iterate along the values of month.name, we can:\n\nfor (i in month.name)\n  print(i)\n\n[1] \"January\"\n[1] \"February\"\n[1] \"March\"\n[1] \"April\"\n[1] \"May\"\n[1] \"June\"\n[1] \"July\"\n[1] \"August\"\n[1] \"September\"\n[1] \"October\"\n[1] \"November\"\n[1] \"December\"\n\n\n\nAvoiding Infinite Loops\nIt is very easy to create an infinite loop when you are working with while loops. Infinite loops never exit, because the condition is always true. If in the while loop example we decrement x instead of incrementing x, the loop will run forever.\nYou want to try very hard to avoid ever creating an infinite loop - it can cause your session to crash.\nOne common way to avoid infinite loops is to create a second variable that just counts how many times the loop has run. If that variable gets over a certain threshold, you exit the loop.\nThis while loop runs until either x &lt; 10 or n &gt; 50 - so it will run an indeterminate number of times and depends on the random values added to x. Since this process (a ‘random walk’) could theoretically continue forever, we add the n&gt;50 check to the loop so that we don’t tie up the computer for eternity.\n\nx &lt;- 0\nn &lt;- 0 # count the number of times the loop runs\n\nwhile (x &lt; 10) { \n  print(x)\n  x &lt;- x + rnorm(1) # add a random normal (0, 1) draw each time\n  n &lt;- n + 1\n  if (n &gt; 50) \n    break # this stops the loop if n &gt; 50\n}\n\n[1] 0\n[1] -0.2001495\n[1] 0.5797077\n[1] 0.8739544\n[1] 1.530348\n[1] 1.02327\n[1] 0.3130435\n[1] -0.8414991\n[1] -1.585043\n[1] -0.5260825\n[1] -1.117144\n[1] -1.950491\n[1] -3.14339\n[1] -1.691602\n[1] -1.175734\n[1] -1.636018\n[1] -2.150056\n[1] -1.816715\n[1] -1.744677\n[1] -1.119064\n[1] -1.228226\n[1] -0.4213547\n[1] 0.1538269\n[1] 0.4886228\n[1] -0.2244994\n[1] -0.4305755\n[1] 0.3185595\n[1] -0.4740908\n[1] -1.236955\n[1] -0.3465337\n[1] 0.6407172\n[1] 0.8371992\n[1] 2.776049\n[1] 0.3709771\n[1] -0.8347131\n[1] -1.49092\n[1] -1.093306\n[1] -0.737781\n[1] -0.5962192\n[1] -1.211619\n[1] -1.748857\n[1] -2.062614\n[1] -1.105728\n[1] -2.572487\n[1] -3.094324\n[1] -3.83098\n[1] -5.192379\n[1] -6.041526\n[1] -4.721258\n[1] -6.843124\n[1] -7.52522\n\n\nIn the example above, there are more efficient ways to write a random walk, but we will get to that later. The important thing here is that we want to make sure that our loops don’t run for all eternity.\nControlling Loops\n\n\nSometimes it is useful to control the statements in a loop with a bit more precision. You may want to skip over code and proceed directly to the next iteration, or, as demonstrated in the previous section with the break statement, it may be useful to exit the loop prematurely.\n\n\nBreak Statement\nNext/Continue Statement\n\n\n\n\n\nA break statement is used to exit a loop prematurely\n\n\n\n\n\nA next (or continue) statement is used to skip the body of the loop and continue to the next iteration\n\n\n\n\n\nLet’s demonstrate the details of next/continue and break statements.\nWe can do different things based on whether i is evenly divisible by 3, 5, or both 3 and 5 (thus divisible by 15)\n\nfor (i in 1:20) {\n  if (i %% 15 == 0) {\n    print(\"Exiting now\")\n    break\n  } else if (i %% 3 == 0) {    \n    print(\"Divisible by 3\")\n    next\n    print(\"After the next statement\") # this should never execute\n  } else if (i %% 5 == 0) {\n    print(\"Divisible by 5\")\n  } else {\n    print(i)\n  }\n}\n\n[1] 1\n[1] 2\n[1] \"Divisible by 3\"\n[1] 4\n[1] \"Divisible by 5\"\n[1] \"Divisible by 3\"\n[1] 7\n[1] 8\n[1] \"Divisible by 3\"\n[1] \"Divisible by 5\"\n[1] 11\n[1] \"Divisible by 3\"\n[1] 13\n[1] 14\n[1] \"Exiting now\"\n\n\n\nTo be quite honest, I haven’t really ever needed to use next/continue statements when I’m programming, and I rarely use break statements. However, it’s useful to know they exist just in case you come across a problem where you could put either one to use."
  },
  {
    "objectID": "00-prereading.html#footnotes",
    "href": "00-prereading.html#footnotes",
    "title": "Pre-reading",
    "section": "",
    "text": "This means that doubles take up more memory but can store more decimal places. You don’t need to worry about this much in R.↩︎\nIn some ways, this is like the difference between an automatic and a manual transmission - you have fewer things to worry about, but you also don’t know what’s going on under the hood nearly as well↩︎\nFrom https://cran.r-project.org/doc/contrib/Short-refcard.pdf↩︎\nThroughout this section (and other sections), lego pictures are rendered using https://www.mecabricks.com/en/workshop. It’s a pretty nice tool for building stuff online!↩︎\nGrumpy cat, Garfield, Nyan cat. Jorts and Jean: The initial post and the update (both are worth a read because the story is hilarious). The cats also have a Twitter account where they promote workers rights.↩︎"
  },
  {
    "objectID": "01-introduction.html#ch1-objectives",
    "href": "01-introduction.html#ch1-objectives",
    "title": "\n1  Introduction\n",
    "section": "Objectives",
    "text": "Objectives\n\nSet up necessary software for this class on personal machines.\nDetect and resolve problems related to file systems, working directories, and system paths when troubleshooting software installation.\nCreate Quarto documents with good reproducible principles."
  },
  {
    "objectID": "01-introduction.html#getting-started-checkins",
    "href": "01-introduction.html#getting-started-checkins",
    "title": "\n1  Introduction\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere are four check-ins for this week:\n\nCheck-in 1.1: Syllabus & Learning Community Quiz\n\n\nComplete the Syllabus & Learning Community Quiz on Canvas.\nThe course syllabus includes information about class dates, course structure, and expectations.\nI will also be asking you some questions about yourself so I can best support you in this course! If you plan to take this quiz more than once (you have up to three retakes on check-ins) you may want to copy your open ended answers into another document so you do not have to retype them each time.\n\nCheck-in 1.2: Discord Server\n\n\nIntroduce yourself in the “introductions” channel of the Discord Server (under COURSE DETAILS).\n\nUsing the Class Discord\nWe’ll be using Discord to interact with our peers and instructor outside of class.\nDiscord is a platform for text chatting, voice chatting, and screen sharing.\nI will often be on Discord when in my office.\nJoin the server\nJoin the Stat 331/531 Server (link to join can be found on canvas) to start experimenting with the interface.\nWhen you join the server, you will be given some suggestions to get started.\n\nI recommend you click through these - and in particular, it is probably a good idea to download the desktop version of Discord, and perhaps to install it on your phone if you wish.\nSet up your account\nVerify your email\nTo use this Discord server, you must have a verified email.\nNobody (including your professors) will be able to see this email, and it does not have to be your Cal Poly email. This is simply to keep the server from being overrun by temporary accounts.\nCreate your identity\nThe first thing you should do is decide what name and picture you would like to use.\n\nI would like to strongly encourage you to use your real name and picture, so that everyone can get to know you. However, if you prefer to remain anonymous, you are free to do so.\n\n(Please do not be like Regina and use the name of another student, however! This kind of impersonation will result in a permanent ban from the server.)\nDecide about privacy and notifications\nThe default settings on the channel are probably just fine for you.\nFeel free to make any changes that work for you, though.\nYou can change your message notifications:\n\nYou can edit your privacy settings, although most things are already private:\n\nConnect other apps\nYou can connect other apps to Discord, either for productivity or just for fun.\n\nUsing the Channels\nThe server is made up of many channels. Some are text chatrooms, while some are “Voice Channels” that connect you via audio to everyone else in the channel.\nText Channels\nUse the #general channel for anything and everything:\n\nIf your question is about course logistics, rather than the material itself, consider using the #class-logistics channel:\n\nYou can use the specific weekly channels to ask questions about the material…\n\n… or the specific lab assignment.\n\nNotice that you can use tick marks (```), like in R Markdown / Quarto, to make your code appear in a formatted code box.\nVoice Channels\nTo join a voice channel, simply click it! Make sure you are careful about when you are muted or unmuted.\n\nThe extra “Side Chat” channels are limited to 4 or 8 people, if you would like to start an impromptu study conversation without being heard by me and / or the rest of the class. (I’ll only drop in if you invite us!)\nVoice channels can also be used for people to “Go Live”, and share their screen with everyone else.\n\nWhile this will usually be something professors use to demonstrate code, you can go live, too! But you may need to download the desktop version of Discord to do so.\nPrivate messages\nIt is also easy to send private messages, to your professor or to each other. These private messages can also easily be used to launch a private video chat and / or screen sharing.\n\nCreating your own server\nLast but not least - for the teams you are a part of, you may want to use Discord to communicate with each other about the weekly assignments. You can do this by creating your own server! You can easily hop between servers during work parties, to ask each other questions or just to take a break and chat about life.\n\nIntroduce yourself in the “introductions” channel of the Discord Server under WEEK 0: BASICS OF CODING. Share your name, major, year in school, and either (1) something you are currently obsessed with or (2) your go to study spot on campus or in SLO. Take a screenshot of your introduction on the discord channel and upload it to the Canvas assignment.\n\nCheck-in 1.3: RStudio Project\n\n\nGet set-up!\nFollow the instructions for Installing R, RStudio, and Quarto. Then create a class directory and RStudio project for this course. You are set up and ready to go!\nTake a screenshot of your class directory, showing the RProject and the folder organization, including a week-1 or practice-activity folder. Upload this screenshot to the Canvas assignment.\n\nCheck-in 1.4: Introduction to R & RStudio"
  },
  {
    "objectID": "01-introduction.html#getting-started",
    "href": "01-introduction.html#getting-started",
    "title": "\n1  Introduction\n",
    "section": "\n1.1 Getting Started",
    "text": "1.1 Getting Started\nSetting up your computer\nIn this section, I will provide you with links to set up various programs on your own machine. If you have trouble with these instructions or encounter an error, post on the class discord or contact me for help.\nHere’s a very broad overview of what each of these programs or services does and why I’m asking you to install them.\n\n\n\n\n\n\n\nProgram\nLogo\nPurpose\n\n\n\nR\n\nA statistical programming language built around working with data\n\n\nRStudio IDE\n\nAn integrated desktop environment created to make it easy to work with R and other data-science programming tools.\n\n\nQuarto\n\nA document creation system based on pandoc. Quarto allows you to include code, results, and pictures generated from data within a document so that they automatically update when the document is recompiled.\n\n\n\nIf you already have R downloaded, please follow these steps anyways, to make sure you have the most recent version of R. Do not ignore these instructions. If you neglect to update your version of R, you may find that updating a package will make it so your code will not run.\nCheck-in 1.3: Installing R, RStudio, and Quarto\n\n\n\nDownload and run the R installer for your operating system from CRAN:\n\nWindows: https://cran.rstudio.com/bin/windows/base/\n\nMac: https://cran.rstudio.com/bin/macosx/ (double check your macOS version)\nLinux: https://cran.rstudio.com/bin/linux/ (pick your distribution)\n\n(Required) If you are on Windows, you should also install the Rtools4 package; this will ensure you get fewer warnings later when installing packages. Make sure you double check the version compatability.\nMore detailed instructions for Windows are available here\n\nDownload and install the latest version of RStudio for your operating system (see Step 2 Installers). RStudio is a integrated development environment (IDE) for R - it contains a set of tools designed to make writing R code easier.\n\nEnsure Quarto is downloaded and installed. Quarto is a command-line tool released by RStudio that allows Rstudio to work with R specific tools in a unified way. We will talk more about Quarto in a later section, but for now just know this is the “notebook” you will be completing and writing all your assignments in.\nThe newest versions of RStudio have started automatically installing Quarto when RStudio is downloaded\nTo check – Open a new Rstudio window &gt; click + in the top left under File &gt; Quarto Document\n\n\n\n\n\n\n\n\n\n\nIf you do not see Quarto, download and install the latest version of Quarto for your operating system.\n Make sure you remember to create your RProject and upload a screenshot to canvas.\n\n\nIf you would like a video tutorial on downloading RStudio, here is one:\n\n\n\n\n\n\n\nRStudio organization has recently re-branded to posit. Rstudio is still the name of the IDE, but these two names may be used interchangeably. See a video of Hadley Wickham talking about the re-branding.\n\n\n\nIntroduction to R and RStudio\nIn this section, we will learn more about some of the tools you just installed. You may have worked with R and RStudio in previous classes, but never had a course dedicated to learning about their functionality.\nIntroduction to R\n\nR is a statistical programming language. Unlike more general-purpose languages, R is optimized for working with data and doing statistics. R was created by Ross Ihaka and Robert Gentleman in 1993 (hence “R”) and was formally released by the R Core Group in 1997 (a group of 20ish volunteers who are the only people who can change the base - built in- functionality of R). If you want to build an independent, standalone graphical interface, or run a web server, R is probably not the ideal language to use (you might want C/python or PHP/python, respectively). If you want to vacuum up a bunch of data, fit several regression models, and then compare the models, R is a great option and will be faster than working in a more general-purpose language like C or base python.\n\n\n\nR is\n\nvector-based\n1 indexed (start counting 1, 2, 3, …)\na scripting language (R code does not have to be compiled before it is run)\n\nOne thing to know about R is that it is open-source. This means that no company owns R (like there is for SAS or Matlab) and that developers cannot charge for the use of their R software. This does not mean that all of your code needs to be public (you can keep your code private), but it is important to be a good open-source citizen by sharing your code publicly when possible (later we will learn about GitHub), contributing to public projects and packages, creating your own packages, and using R for ethical and respectful projects.\n\nNote that RStudio is NOT R, but a platform to help you use R through and that it is a way to make money around the culture of R.\n\n\nThe History of R\n\nThe History of R\nRStudio: the IDE\nAn IDE is an integrated development environment - a fancy, souped up text editor that is built to make programming easier. Back in the dark ages, people wrote programs in text editors and then used the command line to compile those programs and run them.\n\n\n\n\n\n\nRStudio provides a cheat-sheet for the IDE if you are so inclined.\n\nRStudio is not R - it’s just a layer on top of R. So if you have a question about the user interface, you have an RStudio question. If you have a question about the code, you have an R question.\n\nNavigating RStudio\n\n\n\n\nThe RStudio window will look something like this.\n\n\n\n\n\nTop-left\nTop-right\nBottom-left\nBottom-right\n\n\n\nIncludes the text editor. This is where you’ll do most of your work.\n\n\n\n\n\nThe logo on the script file indicates the file type. When an R file is open, there are Run and Source buttons on the top which allow you to run selected lines of code (Run) or source (run) the entire file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\nIn the top right, you’ll find the environment, history, and connections tabs. The environment tab shows you the objects available in R (variables, data files, etc.), the history tab shows you what code you’ve run recently, and the connections tab is useful for setting up database connections.\n\n\nOn the bottom left is the console. There are also other tabs to give you a terminal (command line) prompt, and a jobs tab to monitor progress of long-running jobs. In this class we’ll primarily use the console tab.\n\nknitr::include_graphics(\"https://srvanderplas.github.io/stat-computing-r-python/images/tools/Rstudio-terminal-R.png\")\n\n\n\n\n\n\nOn the bottom right, there are a set of tabs:\n\n\nfiles (to give you an idea of where you are working, and what files are present),\n\n\n\n\n\n\n\n\n\n\nplots (which will be self-explanatory),\npackages (which extensions to R are installed and loaded),\n\n\nknitr::include_graphics(\"https://srvanderplas.github.io/stat-computing-r-python/images/tools/Rstudio-packages-tab.png\")\n\n\n\n\n\nthe help tab (where documentation will show up), and\n\n\n\n\n\n\n\n\n\n\nthe viewer window, which is used for interactive graphics or previewing HTML documents.\n\n\n\n\nInstalling Packages\nOne of R’s strengths is the package repository, CRAN (Comprehensive R Archive Network), that allows anyone (yes, even you!) to write an R package. Packages contain “extra” functionality (outside the base functionality of R). This means that R generally has the latest statistical methods available, and one of the best ways to ensure someone uses your work is to write an R package to make that work accessible to the general population of statisticians/biologists/geneticists.\nTo install the tibble package in R, we would use the following code:\n\ninstall.packages(\"tibble\")\n\nThen, to use the functions within that package, we need to load the package:\n\nlibrary(\"tibble\")\n\nWhen you load a package, all of the functions in that package are added to your R Namespace (this is a technical term) - basically the list of all of the things R knows about. This may be problematic if you have two packages with the same function name.\n\nYou only need to install a package once and your computer will be able to find the package on your computer when it needs to. However, you need to load the package every time R is restarted or you switch to a new project.\n\nIf you want to use a function from a package without loading the package into your namespace, you can do that by using pkgname::function syntax.\nFor instance, this code creates a sample data frame using the tribble() function in the tibble package.\n\ntibble::tribble(~col1, ~col2, 1, 'a', 2, 'b', 3, 'c')\n\n# A tibble: 3 × 2\n   col1 col2 \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 c    \n\n\nTest your setup\nWe will find our way in R and learn more about Quarto in the following sections, but for now open RStudio on your computer and explore a bit.\n\nCan you find the R console? Type in 2+2 to make sure the result is 4.\nRun the following code in the R console:\n\n\ninstall.packages(\n\n  c(\"tidyverse\", \"rmarkdown\", \"knitr\", \"quarto\")\n\n)\n\n\nCan you find the text editor?\n\nCreate a new quarto document (File &gt; New File &gt; Quarto Document).\nCompile the document using the Render button and use the Viewer pane to see the result.\nIf this all worked, you have RStudio, Quarto, and R set up correctly on your machine.\n\n\n\nAdditional Resources: Basics of R and RStudio\nBasics of R Programming\nBasics of R\nRStudio Primer, Basics of Programming in R\nIntroduction to RStudio\nA tour of RStudio, BasicsBasics1\nQuick tour of RStudio"
  },
  {
    "objectID": "01-introduction.html#directories-and-projects",
    "href": "01-introduction.html#directories-and-projects",
    "title": "\n1  Introduction\n",
    "section": "\n1.2 Directories, Paths, and Projects",
    "text": "1.2 Directories, Paths, and Projects\nIn the pre-reading section, File Systems, we learned how to organize our personal files and locate them using absolute and relative file paths. The idea of a “base folder” or starting place was introduced as a working directory. In R, there are two ways to set up your file path and file system organization:\n\n\n\n\nSet your working directory in R (do not reccommend)\nUse RProjects (preferred!)\n\nWorking Directories in R\n\nTo find where your working directory is in R, you can either look at the top of your console or type getwd() into your console.\n\n\n\n\n\n\ngetwd()\n\n[1] \"C:/Users/erobin17/OneDrive - Cal Poly/stat331-calpoly-text\"\n\n\nAlthough it is not recommended, you can set your working directory in R with setwd().\n\nsetwd(\"/path/to/my/assignment/folder\")\n\nRprojects\nSince there are often many files necessary for a project (e.g. data sources, images, etc.), R has a nice built in system for setting up your project organization with RProjects. You can either create a new folder on your computer containing an Rproject (e.g., you have not yet created a folder for this class) or you can add an Rproject to an existing folder on your computer (e.g., you have already created a folder for this class).\n\n\nCreate with a new folder\nAdd to an existing folder\n\n\n\nTo create a Rproject, first open RStudio on your computer and click File &gt; New Project, then:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGive your folder a name (it doesn’t have to be my-stat331). However, it is good practice for this file folder name to not contain spaces.\nThen, browse on your computer for a location to save this folder to. For example, mine is saved in my Documents. Make sure you know how to find this; it should NOT be saved in your Downloads!\n\n\n\n\n\n\n\n\nThis new folder, my-stat331 should now live in your Documents folder (or wherever you save it to) and contain a my-stat331.Rproj file. This is your new “home” base for this class - whenever you refer to a file with a relative path it will begin to look for it here.\n\n\nTo add a Rproject to an existing folder on your computer (e.g., you already created a folder for this class), first open RStudio on your computer and click File &gt; New Project, then:\n\n\n\n\n\n\n\n\nThen, browse on your computer to select the existing folder you wish to add your Rproject to. For example, mine is saved in my Documents and called my-stat331.\n\n\n\n\n\n\n\n\nYour existing folder, my-stat331 should now contain a my-stat331.Rproj file. This is your new “home” base for this class - whenever you refer to a file with a relative path it will begin to look for it here.\n\n\n\n\n\n\nIf you plan to use the Studio computers during class, I recommend having a way to access and save your material between the Studio computers and your personal computer (e.g. put your class directory and Rproject on OneDrive). The idea with Rprojects is then your relative file paths will work in either computer without any changes!\n\nCheck-in 1.3: RProject\n\nTake a screenshot of your class directory, showing the Rproject and the folder organization, including a week-1 or practice-activity folder. Upload this screenshot to the Canvas assignment.\n\nFile Paths in R\n\nA quick warning on file paths is that Mac/Linux and Windows differ in the direction of their backslash to separate folder locations. Mac/Linux use / (e.g. practice-activities/PA1.pdf) while Window’s uses \\ (e.g. practice-activities\\PA1.pdf).\nR can work with both, however, a backslash \\ means something different to R so if you copy a file path from your file filder in Windows, you will need to replace all backslashes with a double backslash \\\\ (e.g. practice-activities\\\\PA1.pdf)\n\nWorkflow\nWorkflow and Projects\nSoftware Carpentry – Project Managment with RStudio"
  },
  {
    "objectID": "01-introduction.html#scripts-and-notebooks",
    "href": "01-introduction.html#scripts-and-notebooks",
    "title": "\n1  Introduction\n",
    "section": "\n1.3 Scripts and Notebooks",
    "text": "1.3 Scripts and Notebooks\nIn this class, we’ll be using markdown notebooks to keep our code and notes in the same place. One of the advantages of both R is that it is a scripting language, but it can be used within notebooks as well. This means that you can have an R script file, and you can run that file, but you can also create a document (like the one you’re reading now) that has code AND text together in one place. This is called literate programming and it is a very useful workflow both when you are learning programming and when you are working as an analyst and presenting results.\nScripts\nBefore I show you how to use literate programming, let’s look at what it replaces: scripts. Scripts are files of code that are meant to be run on their own. They may produce results, or format data and save it somewhere, or scrape data from the web – scripts can do just about anything.\nScripts can even have documentation within the file, using # characters (at least, in R) at the beginning of a line. # indicates a comment – that is, that the line does not contain code and should be ignored by the computer when the program is run. Comments are incredibly useful to help humans to understand what the code does and why it does it.\n\nPlotting a logarithmic spiral\nThis code will use concepts we have not yet introduced - feel free to tinker with it if you want, but know that you’re not responsible for being able to write this code yet. You just need to read it and get a sense for what it does. I have heavily commented it to help with this process.\n\n# Define the angle of the spiral (polar coords)\n# go around two full times (2*pi = one revolution)\ntheta &lt;- seq(0, 4*pi, .01)\n# Define the distance from the origin of the spiral\n# Needs to have the same length as theta\nr &lt;- seq(0, 5, length.out = length(theta))\n\n# Now define x and y in cartesian coordinates\nx &lt;- r * cos(theta)\ny &lt;- r * sin(theta)\n\nplot(x, y, type = \"l\")\n\n\n\nFigure 1.1: A Cartesian Spiral in R\n\n\n\nTo create your first script, click File &gt; New File &gt; R Script and copy paste the code from above. You can save this script on your computer just as you would any other file such as a word document, pdf, or image.\nScripts can be run in Rstudio by clicking the Run button  at the top of the editor window when the script is open.\n\n\n\n\n\n\nMost of the time, you will run scripts interactively - that is, you’ll be sitting there watching the script run and seeing what the results are as you are modifying the script. However, one advantage to scripts over notebooks is that it is easy to write a script and schedule it to run without supervision to complete tasks which may be repetitive.\nNotebooks\nNotebooks are an implementation of literate programming. R has native notebooks that allow you to code in R. This book is written using Quarto markdown, which is an extension of Rmarkdown.\nIn this class, we’re going to use Quarto/R markdown. This matters because the goal is that you learn something useful for your own coding and then you can easily apply it when you go to work as an analyst somewhere to produce impressive documents.\nTo create a quarto document click File &gt; New File &gt; Quarto Document. This will open a notebook template using quarto. You can then Render the document to a pdf or html file.\nIntroduction to Quarto\n(Required) Read through the following resources to introduce Quarto:\n\nR4DS: Quarto\nIntro to Quarto\n\nWhile in this class we will be using Quarto, before Quarto there was RMarkdown (and it is still widely used). If you wish, you can read about Rmarkdown here.\nDownload and save the Markdown syntax Cheat Sheet.\nLearn more about Notebooks and Quarto\nThere are some excellent opinions surrounding the use of notebooks in data analysis:\n\n\nWhy I Don’t Like Notebooks” by Joel Grus at JupyterCon 2018\n\nThe First Notebook War by Yihui Xie (response to Joel’s talk).\nYihui Xie is the person responsible for knitr and Rmarkdown.\n\nYou can learn more about the functionality of Quarto at the following links:\n\nComputations in Quarto\nAuthoring & Formatting Quarto Documents\n\nYou can find the entire list of options you can use to format your HTML file with Quarto here. Poke around the gallery of cool HTML documents rendered with Quarto:\n\nInteractive document published to the web\nAdvanced Layout with HTML\nPimp my RMarkdown\nQuarto Tip A Day"
  },
  {
    "objectID": "01-introduction.html#getting-help",
    "href": "01-introduction.html#getting-help",
    "title": "\n1  Introduction\n",
    "section": "\n1.4 Getting help",
    "text": "1.4 Getting help\nIn R, you can access help with a ?. Suppose we want to get help on a for loop. In the R console, we can run this line of code to get help on for loops.\n\n?`for`\n\nstarting httpd help server ... done\n\n\nBecause for is a reserved word in R, we have to use backticks (the key above the TAB key) to surround the word for so that R knows we’re talking about the function itself. Most other function help can be accessed using ?function_name.\n(You will have to run this in interactive mode for it to work)\nw3schools has an excellent R help on basic functions that may be useful as well - usually, these pages will have examples.\nGoogle is your friend.\nThe R community has an enormous aresenal of online learning resources. I will linked a few throughout the “read more” sections in this text, but you can always find more!\n\nLearn to:\n\nGoogle for tutorials and examples\nUse Stack Overflow\n\nAsk questions on Twitter\nMake good use of the vast and welcoming R network on the internet\n\nCheck-in 1.4: Introduction to R & RStudio\nAnswer the following multiple choice questions on the Canvas Quiz.\nQuestion 1: What does it mean for R to be “open-source”?\nQuestion 2: How often do you need to install a package on your computer?\nQuestion 3: What is the tidyverse?\nQuestion 4: What is the purpose of RStudio in relation to R?\nDirectories, paths, and projects – use the following image to answer the next three questions.\n\n\n\n\n\nQuestion 5: When working with R projects, what is the purpose of the “.Rproj” file?\nQuestion 6: In what order would you ‘enter’ the files to access my-data.csv using a relative file path from your my-331-class as “home base”?\nQuestion 7: When naming folders, sub-folders, and files, it is good practice to use spaces."
  },
  {
    "objectID": "01-introduction.html#pa-1-find-the-mistakes",
    "href": "01-introduction.html#pa-1-find-the-mistakes",
    "title": "\n1  Introduction\n",
    "section": "PA 1: Find the Mistakes",
    "text": "PA 1: Find the Mistakes\nYou can access PA1: Find the Mistakes one of two ways:\n\nClick here to access an RStudio Cloud project which we work on in groups on Day 1 of the course (Note: if you do not have an RStudio (Posit) Cloud account, you will be asked to create one). Make sure to save a permanent copy!\nIf you have already installed R, RStudio, and Quarto, you can create a new Quarto document, and copy the practice activity template into the file (Note: make sure your quarto file is on Source mode and not Visual mode when you copy the template).\n\nThe components of the Practice Activity are described below:\nPart One:\nThis file has many mistakes in the code. Some are errors that will prevent the file from knitting; some are mistakes that do NOT result in an error.\nFix all the problems in the code chunks.\nPart Two:\nFollow the instructions in the file to uncover a secret message.\nSubmit the name of the poem as the answer to the Canvas Quiz question."
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#ch2-objectives",
    "href": "02-tidy-data-and-basics-of-graphics.html#ch2-objectives",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "Objectives",
    "text": "Objectives\n\nRecognize tidy data formats and identify the variables and columns in data sets\nRead in data from common formats into R\nDescribe charts using the grammar of graphics\nCreate layered graphics that highlight multiple aspects of the data\nEvaluate existing charts and develop new versions that improve accessibility and readability"
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#ch2-checkins",
    "href": "02-tidy-data-and-basics-of-graphics.html#ch2-checkins",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere are two check-ins for this week:\n\nCheck-in 2.1: Loading in Data\nCheck-in 2.2: Introduction to Data Visualization with ggplot"
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#tidy-data",
    "href": "02-tidy-data-and-basics-of-graphics.html#tidy-data",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "\n2.1 Tidy Data",
    "text": "2.1 Tidy Data\nIn the pre-reading, we learned about Basic Data Types (strings/characters, numeric/double/floats, integers, and logical/booleans) and Data Structures (1D - vectors and lists; 2D - matrices and data frames). This class will mainly focus on working with data frames. It is important to know the type and format of the data you’re working with. It is much easier to create data visualizations or conduct statistical analyses if the data you use is in the right shape.\nData can be formatted in what are often referred to as wide format or long format. Wide format data has variables spread across columns and typically uses less space to display. This format is how you would typically choose to present your data as there is far less repetition of labels and row elements. Long format data is going to have each variable in a column and each observation in a row; this is likely not the most compact form of the data.\nLong formatted data is often what we call tidy data - a specific format of data in which each variable is a column, each observation is a row and each type of observational unit forms a table (or in R, a data frame).\n\n\n\n\nWhat is tidy data? Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst\n\n\n\nSame data, different layouts\nCan you determine which of the following data sets follows the tidy data format?\n\n\nOption 1:\nOption 2:\nOption 3:\nSolution\n\n\n\n\n\nName\nTreatment A\nTreatment B\n\n\n\nBrian Boatwright\nNA\n18\n\n\nTrenna Porras\n4\n1\n\n\nHannaa Kumar\n6\n7\n\n\n\n\n\n\n\nTreatment\nBrian Boatwright\nTrenna Porras\nHannaa Kumar\n\n\n\nA\nNA\n4\n6\n\n\nB\n18\n1\n7\n\n\n\n\n\n\n\nName\nTreatment\nMeasurement\n\n\n\nBrian Boatwright\nA\nNA\n\n\nTrenna Porras\nA\n4\n\n\nHannaa Kumar\nA\n6\n\n\nBrian Boatwright\nB\n18\n\n\nTrenna Porras\nB\n1\n\n\nHannaa Kumar\nB\n7\n\n\n\n\n\nOption 3 follows the tidy data format since each variable (Name, Treatment, and Measurement) belong to their own columns and each observation taken is identified by a single row.\n\n\n\nData frames are a specific object type in R, data.frame(), and can be indexed the same as matrices. It may be useful to actually look at your data before beginning to work with it to see the format of the data. The following functions in R help us learn information about our data sets:\n\n\nclass(): outputs the object type\n\nnames(): outputs the variable (column) names\n\nhead(): outputs the first 6 rows of a dataframe\n\nglimpse() or str(): output a transpose of dataframe or matrix; shows the data types\n\nsummary() outputs 6-number summaries or frequencies for all variables in the data set depending on the variables data type\n\ndata$variable: extracts a specific variable (column) from the data set\n\nYou may also choose to click on the data set name in your Environment window pane in R and the data set will pop up in a new tab in the script pane.\n\nWorking with data sets in R\nThe cars data set is a default data set that lives in R (for examples like this!).\n\n# outputs the class of the cars data set (data.frame)\nclass(cars)\n\n[1] \"data.frame\"\n\n# outputs the names of the variables included in the cars data set (speed, dist)\nnames(cars)\n\n[1] \"speed\" \"dist\" \n\n# outputs the first six rows of the cars data set\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n# outputs\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n# outputs the second row of the cars data set\ncars[2,]\n\n  speed dist\n2     4   10\n\n# outputs the first column of the cars data set as a vector\ncars[,1]\n\n [1]  4  4  7  7  8  9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15\n[26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25\n\n# outputs the speed variable of the cars data set as a vector (notice this is the same as above)\ncars$speed\n\n [1]  4  4  7  7  8  9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15\n[26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25\n\n\n:::\nIn a perfect world, all data would come in the right format for our needs, but this is often not the case. We will spend the next few weeks learning about how to use R to reformat our data to follow the tidy data framework and see why this is so important. For now, we will work with nice clean data sets but you should be able to identify when data follows a tidy data format and when it does not.\n\n2.1.0.1 Wait. So is it Tidy format?\nThe concept of tidy data is useful for mapping variables from the data set to elements in a graph, specifications of a model, or aggregating to create summaries. However, what is considered to be “tidy data” format for one task, might not be in the correct “tidy data” format for a different task. It is important for you to consider the end goal when restructuring your data.\nPart of this course is building the skills for you to be able to map your data operation steps from an original data set to the correct format (and output)."
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#loading-external-data",
    "href": "02-tidy-data-and-basics-of-graphics.html#loading-external-data",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "\n2.2 Loading External Data",
    "text": "2.2 Loading External Data\n\n2.2.1 An Overview of External Data Formats\nIn order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we’ll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables (like tidy data we learned about above!). This type of data can be stored on the computer in multiple ways:\n\n\nas raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable.\n\n\n.csv “Comma-separated”\n\n.txt plain text - could be just text, comma-separated, tab-separated, etc.\n\n\n\nin a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not completely binary formats, but they’re also not raw text files either. They’re a hybrid - a special type of markup that is specific to the filetype and the program it’s designed to work with. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet.\n\n.xls\n.xlsx\n\n\n\n\nThere is a collection of spreadsheet horror stories here and a series of even more horrifying threads here.\nAlso, there’s this amazing comic:\n\nTo be minimally functional in R, it’s important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files.\nLearn more about external data\n\nAdditional external data formats\nWe will not cover binary files and databases, but you can consult one or more online references if you are interested in learning about these.\n\n\nas a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected.\n\nR, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as foreign in R will let you read data from other programs, and packages such as haven in R will let you write data into binary formats used by other programs. We aren’t going to do anything with binary formats, just know they exist.\n\n\n\nin a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with.\n\nThere are, of course, many other non-tabular data formats – some open and easy to work with, some inpenetrable. A few which may be more common:\n\nWeb related data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis.\nSpatial files: Shapefiles are by far the most common version of spatial files1. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information.\n\n2.2.2 Where does my data live?\n\n\n\nThe most common way to read data into R is with the read_csv() function. You may provide the file input as either a url to the data set or a path for the file input.\n\nThe read_csv() function belongs to the readr package (side note: readr is installed and loaded as part of the tidyverse package). To install, either click on your Packages tab or use:\n\ninstall.packages(\"readr\")\n\n\n\nread_csv()\n\n# load the readr package (or alternatively the tidyverse package)\nlibrary(readr)\n\n# this will work for everyone!\nsurveys &lt;- read_csv(file = \"https://raw.githubusercontent.com/earobinson95/stat331-calpoly/master/lab-assignments/Lab2-graphics/surveys.csv\")\n\n# this will work only on my computer\n# notice this is an absolute file path on my computer\nsurveys &lt;- read_csv(file = \"C:/Users/erobin17/OneDrive - Cal Poly/stat331/labs/lab2/surveys.csv\")\n\nRecall our discussion about Directories, Paths, and Projects. Often you will specify a relative file path to your data set rather than an absolute file path. This works if the file is in the same directory as the code or within a sub-folder of the same directory as the code.\nRelative file paths\n\n# this will work if the file is in the same directory as the code\n# (i.e., the Quarto document and the data are in the same folder)\nsurveys &lt;- read_csv(file = \"surveys.csv\")\n\n# this will work if a sub-folder called \"data\" is in the same directory as the code\n# (i.e., you first have to enter the data sub folder and then you can access the surveys.csv data set)\nsurveys &lt;- read_csv(file = \"data/surveys.csv\")\n\n\n\n\nI often organize my workflow with sub-folders so that I would have a sub-folder called data to reference from my base directory location.\nYou can either choose to create an overall data sub-folder right within your stat331 folder (e.g., stat331 &gt; data) and store all of your data for your class there… or you may choose to store the data associated with each individual assignment within that folder (e.g., stat331 &gt; labs &gt; lab2 &gt; data).\n\n\n\nBut wait! I thought we created our RProjects (my-stat331.Rproj) to indicate our “home base” directory? When working with a Quarto document, this changes your base directory (for any code running within the .qmd file) to the same folder the .qmd file lives in.\nOpen your class directory my-stat331.Rproj and type getwd() into the Console. This should lead you to the folder directory your RProject was created in.\nNow open your lab1.qmd assignment and type getwd() into a code chunk. This should lead you to the folder directory your lab1.qmd file is saved in.\nUgh. Well that is confusing.\n\nA great solution to consistency in file paths is the here package:\n\ninstall.packages(\"here\")\n\n\n\n\n\n\n\nWorkflows that shred…, by Allison Horst\n\n\n\nThis package thinks “here” is the your directory folder the RProject (my-331.Rproj) lives in (e.g., your stat331 folder). This makes a global “home base” that overrides any other directory path (e.g., from your .qmd files).\n\n# shows you the file path the function here() will start at.\nhere::dr_here()\n\nThe here package\nIf my stat331 file structure looks like this:\n\n\n\n\n\nthen my RProject sets the working directory to this stat331 folder. However, if my lab1.qmd file lives inside my labs folder – e.g., C:/Users/erobin17/OneDrive - Cal Poly/stat331/labs/lab1.qmd, then when I try to load data (surveys.csv) in within the lab1.qmd file, it will be looking for that relative file path from within the labs folder (where the lab1.qmd lives) and not from within the stat331 (“home base”) folder.\n\nsurveys &lt;- read_csv(\"surveys.csv\")\n\nHowever, if my data is stored within a data sub-folder as shown above, then within my lab1.qmd file, my relative file path must “backtrack” (..) out of the labs folder one move before entering into the data folder to access the data set.\n\nsurveys &lt;- read_csv(\"../data/surveys.csv\")\n\nEnter the here package! With the here package, within my lab1.qmd file I can ALWAYS assume I am in my stat331 (“home base”) folder and enter directly into the data sub-folder.\n\nsurveys &lt;- read_csv(here::here(\"data\", \"surveys.csv\"))\n\nNote the folder and file names are in quotations because they are names of files and not objects in R.\n\n\n\nUsing “::” before the function (e.g., PackageName::FunctionName) is a way of telling R which package the function lives in without having to load that entire package (e.g., library(PackageName). There are some common functions the R community does this for mainly just out of practice. here is one of them and you will often see here::here() telling you to go into the here package and access/use the here() function. Essentially it saves memory from not having to load unnecessary functions if you only need one function from that package.\n\n\n\nI have a complicated relationship with the here package. I see the strong uses for it, but I don’t always set up my workflow in this way. You will form your own opinions, but the important thing for you to know is how to find the correct file path to read your data into your .qmd files.\n\n\n2.2.3 How do I load in my data?\n\nIt may be helpful to save this data import with the tidyverse cheat sheet.\n\n(required) Read the following to learn more about importing data into R\n\nR4DS: Data Import\n\n\nPreviously in this chapter, we learned about common types of data files and briefly introduced the read_csv() and function. There are many functions in R that read in specific formats of data:\n\n\nBase R\nreadr & readxl\n\n\n\nBase R contains the read.table() and read.csv() functions. In read.table() you must specify the delimiter with the sep = argument. read.csv() is just a specific subset function of read.table() that automatically assumes a comma delimiter.\n\n# comma delimiter\nread.table(file = \"ages.txt\", sep = \",\")\n\n# tab delimiter\nread.table(file = \"ages.txt\", sep = \"\\t\")\n\n\n\nThe tidyverse has some cleaned-up versions in the readr and readxl packages:\n\n\nread_csv() works like read.csv, with some extra stuff\n\nread_txv() is for tab-separated data\n\nread_table() is for any data with “columns” (white space separating)\n\nread_delim() is for special “delimiters” separating data\n\nread_xlsx() is specifically for dealing with Excel files\n\n\n\n\n\nLearn more\n\nRSQLite vignette\n\nSlides from Jenny Bryan’s talk on spreadsheets (sadly, no audio. It was a good talk.)\nThe vroom package works like read_csv but allows you to read in and write to many files at incredible speeds.\n\nCheck-in 2.1: Loading in Data\nFor this check-in you are asked to work through reading in different data sets. The folder Age_Data contains several data sets with the names and ages of five individuals. The data sets are stored as different file types. Download Ages_data.zip here, make sure to unzip the folder, save these in a reasonable place (e.g., stat331 &gt; check-ins &gt; checkin2.1 or stat331 &gt; week2), then use the readr and readxl packages to complete the following and fill in the “gap” code in the Canvas quiz:\n\n\nMy file folder structure looks like this:\n\n\nLoad the appropriate packages for reading in data.\nRead in the data set ages.csv\n\nRead in the data set ages_tab.txt\n\nRead in the data set ages_mystery.txt\n\nRead in the data set ages.xlsx\n\nFind a way to use read_csv() to read ages.csv with the variable “Name” as a factor data type and “Age” as a character data type."
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#basics-of-graphics",
    "href": "02-tidy-data-and-basics-of-graphics.html#basics-of-graphics",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "\n2.3 Basics of Graphics",
    "text": "2.3 Basics of Graphics\nNow that we understand the format of tidy data and how to load external data into R, we want to be able to do something with that data! We are going to start with creating data visualizations (arguably my favorite!).\nThere are a lot of different types of charts, and equally many ways to categorize and describe the different types of charts.\n\nThis is one of the less serious schemes I’ve seen\nBut, in my opinion, Randall missed the opportunity to put a pie chart as Neutral Evil.\n\nHopefully by the end of this, you will be able to at least make the charts which are most commonly used to show data and statistical concepts.\n\n2.3.1 Why do we create graphics?\n\nThe greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (“Why, that scatter diagram has a hole in the middle!”) – John Tukey, Data Based Graphics: Visual Display in the Decades to Come\n\nFundamentally, charts are easier to understand than raw data.\nWhen you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the “real world” into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is “tangible” in some way – if not physical and literally touch-able, at least something we can view and “wrap our heads around”.\nConsider this thought experiment: You have a simple data set - 2 numeric variables, 500 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options:\n\nPrint out the data set\n\n\nhead(simple_data)\n\n# A tibble: 6 × 2\n   var1  var2\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.975  16.9\n2 6.10  178. \n3 4.61  103. \n4 6.49  173. \n5 6.60  190. \n6 6.45  170. \n\n\n\nCreate some summary statistics of each variable and perhaps the covariance between the two variables\n\n\nsummary(simple_data)\n\n      var1              var2       \n Min.   :0.03114   Min.   :-35.87  \n 1st Qu.:2.42756   1st Qu.: 31.48  \n Median :5.10491   Median :103.93  \n Mean   :4.98367   Mean   :132.78  \n 3rd Qu.:7.29845   3rd Qu.:219.89  \n Max.   :9.97662   Max.   :432.62  \n\ncov(simple_data$var1, simple_data$var2)\n\n[1] 324.7271\n\n\n\nDraw a scatter plot of the two variables\n\n\nggplot(data = simple_data,\n       mapping = aes(x = var1,\n                     y = var2\n                     )\n       ) +\n  geom_point()\n\n\n\n\n\n\n\nWhich one would you rather use? Why?\nOur brains are very good at processing large amounts of visual information quickly. Evolutionarily, it’s important to be able to e.g. survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we’re not nearly as equipped to process quickly (and in parallel).\nIt’s a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out.\n\nYou’re going to learn how to make graphics by finding sample code, changing that code to match your data set, and tweaking things as you go. That’s the best way to learn this, and while ggplot has a structure and some syntax to learn, once you’re familiar with the principles, you’ll still want to learn graphics by doing it.\n\n\n\n2.3.2 ggplot2\nIn this class, we’re going to use the ggplot2 package to create graphics in R. This package is already installed as part of the tidyverse, but can be installed:\n\ninstall.packages(\"ggplot2\")\n\nand/or loaded:\n\nlibrary(\"ggplot2\")\n\n# alternatively\nlibrary(\"tidyverse\") # (my preference!)\n\n\n\n\n\nBuilding a masterpiece, by Allison Horst\n\nWe will learn about all of these different pieces and the process of creating graphics by working through examples, but there is a general “template” for creating graphics in ggplot:\n\nggplot(data = &lt;DATA&gt;,\n       mapping = aes(&lt;MAPPINGS&gt;)\n       ) +\n  &lt;GEOM FUNCTION&gt;() +\n  any other arugments ...\n\nwhere\n\n is the name of the data set\n is the name of the geom you want for the plot (e.g., geom_histogram(), geom_line(), geom_point())\n is where variables from the data are mapped to parts of the plot (e.g., x = speed, y = distance, color = carmodel)\nany other arguments could include the theme, labels, faceting, etc.\n\nYou may want to download and save cheat sheets and reference guides for ggplot.\n\n2.3.3 The Grammar of Graphics\nThe grammar of graphics is an approach first introduced in Leland Wilkinson’s book (Wilkinson 2005). Unlike other graphics classification schemes, the grammar of graphics makes an attempt to describe how the data set itself relates to the components of the chart.\nThis has a few advantages:\n\nIt’s relatively easy to represent the same data set with different types of plots (and to find their strengths and weaknesses)\nGrammar leads to a concise description of the plot and its contents\nWe can add layers to modify the graphics, each with their own basic grammar (just like we combine sentences and clauses to build a rich, descriptive paragraph)\n\n\n\nA pyramid view of the major components of the grammar of graphics, with data as the base, aesthetics building on data, scales building on aesthetics, geometric objects, statistics, facets, and the coordinate system at the top of the pyramid. Source: (Sarkar 2018)\n\n\nI have turned off warnings for all of the code chunks in this chapter. When you run the code you may get warnings about e.g. missing points - this is normal, I just didn’t want to have to see them over and over again - I want you to focus on the changes in the code.\n\nExploratory Data Analysis with the grammar of graphics\n\n\nSketch\nggplot\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\nlibrary(ggplot2)\ndata(txhousing) # this data set is housed internally to R\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median\n           )\n       ) + \n  geom_point()\n\n\n\n\n\n\n\nWhen creating a grammar of graphics chart, we start with the data (this is consistent with the data-first tidyverse philosophy).\n\nIdentify the dimensions/aspects of your data set you want to visualize.\n\nDecide what aesthetics you want to map to different variables. For instance, it may be natural to put time on the \\(x\\) axis, or the experimental response variable on the \\(y\\) axis. You may want to think about other aesthetics, such as color, size, shape, etc. at this step as well.\n\nIt may be that your preferred representation requires some summary statistics in order to work. At this stage, you would want to determine what variables you feed in to those statistics, and then how the statistics relate to the geoms (geometric objects) that you’re envisioning. You may want to think in terms of layers - showing the raw data AND a summary geom (e.g., points on a scatter plot WITH a line of best fit overlaid).\n\n\nDecide the geometric objects (geoms) you want to represent your variables with on the chart. For example, a scatterplot displays the information as points by mapping the \\(x\\) and \\(y\\) aesthetics to points on the graph.\nIn most cases, ggplot will determine the scale for you, but sometimes you want finer control over the scale - for instance, there may be specific, meaningful bounds for a variable that you want to directly set.\nCoordinate system: Are you going to use a polar coordinate system? (Please say no, for reasons we’ll get into later!)\nFacets: Do you want to show subplots based on specific categorical variable values?\n\n(this list modified from Sarkar (2018)).\n\nLet’s explore the txhousing data a bit more thoroughly by adding some complexity to our chart. This example will give me an opportunity to show you how an exploratory data analysis might work in practice, while also demonstrating some of ggplot2’s features.\nBefore we start exploring, let’s add a title and label our axes, so that we’re creating good, informative charts:\nxlab(), ylab(), and ggtitle().\nAlternatively, you could use labs(title = , x = , y = ) to add a title and label the axes.\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median)\n       ) + \n  geom_point() + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\") \n\n\n\n\nFirst, we may want to show some sort of overall trend line. We can start with a linear regression, but it may be better to use a loess smooth (loess regression is a fancy weighted average and can create curves without too much additional effort on your part).\ngeom_smooth(method = \"&lt;LINE TYPE&gt;\") where method = lm adds a linear regression model and method = loess adds a loess regression smoother.\n\n\nSketch\nggplot\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median)\n       ) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median)\n       ) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nLooking at the plots here, it’s clear that there are small sub-groupings (see, for instance, the almost continuous line of points at the very top of the group between 2000 and 2005). Let’s see if we can figure out what those additional variables are…\nAs it happens, the best viable option is City.\n...aes(...color = &lt;VARIABLE&gt;)\n\n\nSketch\nggplot\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median, \n           color = city \n           )\n       ) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nThat’s a really crowded graph! It’s slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median, \n           color = city\n           )\n       ) + \n  # geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nIn reality, though, you should not ever map color to something with more than about 7 categories if your goal is to allow people to trace the category back to the label. It just doesn’t work well perceptually.\nSo let’s work with a smaller set of data: Houston, Dallas, Fort worth, Austin, and San Antonio (the major cities). Don’t worry too much about the subsetting code yet, we will get to that next week!\n\ncitylist &lt;- c(\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\")\nhousingsub &lt;- txhousing |&gt;\n  filter(city %in% citylist)\n\nggplot(data = housingsub, #&lt;&lt; \n       aes(x = date, \n           y = median, \n           color = city\n           )\n       ) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices (subset of cities)\")\n\n\n\n\nInstead of using color, another way to show this data is to plot each city as its own subplot. In ggplot2 lingo, these subplots are called “facets”. In visualization terms, we call this type of plot “small multiples” - we have many small charts, each showing the trend for a subset of the data.\nfacet_wrap(), facet_grid()\nHere’s the facetted version of the chart:\n\nggplot(data = housingsub, \n       aes(x = date, \n           y = median\n           )\n       ) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  facet_wrap(~ city) + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices (subset of cities)\")\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\nNow that we’ve simplified our charts a bit, we can explore a couple of the other quantitative variables by mapping them to additional aesthetics:\n\nggplot(data = housingsub, \n       aes(x = date, \n           y = median, \n           size = sales)\n       ) + \n  # Make points transparent\n  geom_point(alpha = 0.15) +  \n  geom_smooth(method = \"loess\") + \n  facet_wrap(~ city) +\n  # Remove extra information from the legend - \n  # line and error bands aren't what we want to show\n  # Also add a title\n  guides(size = guide_legend(title = 'Number of Sales',  \n                             override.aes = list(linetype = NA,  \n                                                 fill = 'transparent' \n                                                 ) \n                             ) \n         ) + #&lt;&lt; \n  # Move legend to bottom right of plot\n  theme(legend.position = c(1, 0), #&lt;&lt; \n        legend.justification = c(1, 0) \n        ) + #&lt;&lt; \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices (subset of cities)\")\n\n\n\n\nUp to this point, we’ve used the same position information - date for the \\(x\\) axis, median sale price for the \\(y\\) axis. Let’s switch that up a bit so that we can play with some transformations on the \\(x\\) and \\(y\\) axis and add variable mappings to a continuous variable.\n\nggplot(data = housingsub, \n       aes(x = listings, \n           y = sales, \n           color = city)\n       ) + \n  geom_point(alpha = 0.15) +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Number of Listings\") + \n  ylab(\"Number of Sales\") + \n  ggtitle(\"Texas Housing Prices (subset of cities)\")\n\n\n\n\nThe points for Fort Worth are compressed pretty tightly relative to the points for Houston and Dallas. When we get this type of difference, it is sometimes common to use a log transformation2. Here, I have transformed both the x and y axis, since the number of sales seems to be proportional to the number of listings.\nscale_x_log10(), scale_y_log10()\n\nggplot(data = housingsub, \n       aes(x = listings, #&lt;&lt; \n           y = sales, #&lt;&lt; \n           color = city\n           )\n       ) + \n  geom_point(alpha = 0.15) +\n  geom_smooth(method = \"loess\") +\n  scale_x_log10() + #&lt;&lt; \n  scale_y_log10() + #&lt;&lt; \n  xlab(\"Number of Listings\") + \n  ylab(\"Number of Sales\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nFor the next demonstration, let’s look at just Houston’s data. We can examine the inventory’s relationship to the number of sales by looking at the inventory-date relationship in \\(x\\) and \\(y\\), and mapping the size or color of the point to number of sales.\n\nhouston &lt;- txhousing |&gt; \n  filter(city == \"Houston\")\n\nggplot(data = houston,\n       aes(x = date, \n           y = inventory, \n           size = sales \n           )\n       ) + \n  geom_point(shape = 1) + \n  xlab(\"Date\") + \n  ylab(\"Months of Inventory\") + \n  guides(size = guide_legend(title = \"Number of Sales\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\nggplot(data = houston, \n       aes(x = date, \n           y = inventory, \n           color = sales \n           )\n       ) + \n  geom_point() + \n  xlab(\"Date\") + \n  ylab(\"Months of Inventory\") + \n  guides(size = guide_colorbar(title = \"Number of Sales\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\nWhich is easier to read?\nWhat happens if we move the variables around and map date to the point color?\n\nggplot(data = houston, \n       aes(x = sales, #&lt;&lt; \n           y = inventory, \n           color = date \n           )\n       ) + \n  geom_point() + \n  xlab(\"Number of Sales\") + \n  ylab(\"Months of Inventory\") + \n  guides(size = guide_colorbar(title = \"Date\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\nIs that easier or harder to read?\n\n\n\nSpecial properties of aesthetics\nGlobal vs Local aesthetics\nAny aesthetics assigned to the mapping within the first line, ggplot(), will be inherited by the rest of the geometric, geom_xxx(), lines. This is called a global aesthetic.\n\nggplot(data = housingsub, \n       mapping = aes(x = date, \n                     y = median, \n                     color = city \n                     )\n       ) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nAny aesthetics assigned to the mapping within a geometric object is only applied to that specific geom. Notice how color is no longer mapped to the regression line and there is only one overall regression line for all cities.\n\nggplot(data = housingsub, \n       mapping = aes(x = date, \n                     y = median\n                     )\n       ) +\n  geom_point(mapping = aes(color = city) \n             ) +\n  geom_smooth(method = \"lm\")\n\n\n\n\nAssigning your aesthetics vs Setting your aesthetics\nWhen you assign a variable from your data set to your aesthetics, you put it inside aes() without quotation marks around the variable name:\n\nggplot(data = housingsub) +\n  geom_point(mapping = aes(x = date, \n                           y = median, \n                           color = city \n                           )\n             )\n\n\n\n\nIf you want to set a specific color, you could put this outside the aes() mapping in quotation marks because blue is not an object in R. Notice this color is assigned specifically in that geometric object.\n\nggplot(data = housingsub) +\n  geom_point(mapping = aes(x = date, \n                           y = median\n                           ), \n             color = \"blue\" \n             )\n\n\n\n\nYou should NOT put your color inside the aes() parentheses.\n\nggplot(data = txhousing) +\n  geom_point(mapping = aes(x = date, \n                           y = median, \n                           color = \"blue\" \n                           )\n             )\n\n\n\n\n\n2.3.3.1 What type of chart to use?\nIt can be hard to know what type of chart to use for a particular type of data. I recommend figuring out what you want to show first, and then thinking about how to show that data with an appropriate plot type. Consider the following factors:\n\nWhat type of variable is x? Categorical? Continuous? Discrete?\nWhat type of variable is y?\nHow many observations do I have for each x/y variable?\nAre there any important moderating variables?\nDo I have data that might be best shown in small multiples? E.g. a categorical moderating variable and a lot of data, where the categorical variable might be important for showing different features of the data?\n\nOnce you’ve thought through this, take a look through cataloges like the R Graph Gallery to see what visualizations match your data and use-case. ### Creating Good Charts\nA chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g. the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate.\nUnfortunately, while our visual system is amazing, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms.\n\n2.3.3.2 Perceptual and Cognitive Factors\nColor\nOur eyes are optimized for perceiving the yellow/green region of the color spectrum. Why? Well, our sun produces yellow light, and plants tend to be green. It’s pretty important to be able to distinguish different shades of green (evolutionarily speaking) because it impacts your ability to feed yourself. There aren’t that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum.\n\n\nSensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons)\n\nNot everyone perceives color in the same way. Some individuals are colorblind or color deficient. We have 3 cones used for color detection, as well as cells called rods which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, &lt;1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances.\n\nYou can take a test designed to screen for colorblindness here\nYour monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. A different test is available here.\n\n In reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown.\n\nIt is possible to simulate the effect of color blindness and color deficiency on an image.\n\n\n\n\n\nOriginal image using a rainbow color scale\n\n\n\n\n\nRed cone deficient\n\n\n\n\n\nGreen cone deficient\n\n\n\n\n\nBlue cone deficient\n\n\n\n\n\n\n\nRed cone absent\n\n\n\n\n\nGreen cone absent\n\n\n\n\n\nBlue cone absent\n\n\n\nIn addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context.\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\nOur brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly “remove” the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown above - because we’re correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade.\nImplications and Guidelines\n\nDo not use rainbow color gradient schemes - because of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance.\nAvoid any scheme that uses green-yellow-red signaling if you have a target audience that may include colorblind people.\nTo “colorblind-proof” a graphic, you can use a couple of strategies:\n\ndouble encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out\nIf you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it!\nIf you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -&gt; dark by colorblind people, so it will be correctly perceived no matter what color you use.\nIf you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is purple - white - orange. In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly.\n\n\nBe conscious of what certain colors “mean”\n\nLeveraging common associations can make it easier to read a color scale and remember what it stands for (e.g. blue for cold, orange/red for hot is a natural scale, red = Republican and blue = Democrat in the US, white -&gt; blue gradients for showing rainfall totals)\nSome colors can can provoke emotional responses that may not be desirable.3\n\nIt is also important to be conscious of the social baggage that certain color schemes may have - the pink/blue color scheme often used to denote gender can be unnecessarily polarizing, and it may be easier to use a colder color (blue or purple) for men and a warmer color (yellow, orange, lighter green) for women4.\n\n\nThere are packages such as RColorBrewer and dichromat that have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer). You can also take a look at other ways to find nice color palettes.\nShort Term Memory\nWe have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time.\nTry it out!\n\nClick here, read the information, and then click to hide it.\n\n1 4 2 2 3 9 8 0 7 8\n\nWait a few seconds, then expand this section\n\nWhat was the third number?\nWithout rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 “bits” of information.\nIn charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart.\nWhere possible, try to keep your legends to 6 or 7 characteristics.\nImplications and Guidelines\n\n\nLimit the number of categories in your legends to minimize the short term memory demands on your reader.\n\nWhen using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude.\n\n\nUse colors and symbols which have implicit meaning to minimize the need to refer to the legend.\nAdd annotations on the plot, where possible, to reduce the need to re-read captions.\nGrouping and Sense-making\nImposing order on visual chaos.\n\n\nAmbiguous Images\nIllusory Contours\nFigure/Ground\n\n\n\nWhat does the figure below look like to you?\n\n\nIs it a rabbit, or a duck?\n\nWhen faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options.\n\n\n\n\nConsider this image - what do you see?\n\nDid you see something like “3 circles, a triangle with a black outline, and a white triangle on top of that”? In reality, there are 3 angles and 3 pac-man shapes. But, it’s much more likely that we’re seeing layers of information, where some of the information is obscured (like the “mouth” of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience.\n\n\nNow, look at the logo for the Pittsburgh Zoo.\n\nDo you see the gorilla and lionness? Or do you see a tree? Here, we’re not entirely sure which part of the image is the figure and which is the background.\n\n\n\nThe ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs.\nThe catchphrase of Gestalt psychology is\n\nThe whole is greater than the sum of the parts\n\nThat is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene.\n\n\nThe Gestalt Heuristics help us to impose order on ambiguous visual stimuli\n\nYou can read about the gestalt rules here, but they are also demonstrated in the figure above.\nIn graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving “with” the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders.\n\nSuppose I want to emphasize the change in the murder rate between 1980 and 2010.\nI could use a bar chart (showing only the first 4 states alphabetically for space)\n\n\n\n\n\nOr, I could use a line chart\n\n\n\n\n\nOr, I could use a box plot\n\n\n\n\n\nWhich one best demonstrates that in every state and region, the murder rate decreased?\nThe line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g. almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn’t allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can’t see the individual relationships.\n\nThe aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) can easily make when examining those plots.^[See this paper for more details.\nGeneral guidelines for accuracy\nThere are certain tasks which are easier for us relative to other, similar tasks.\n\n\n\n\nWhich of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. In fact, the line lengths are the same in both panels.\n\n\n\nWhen making judgments corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level.5\n\nPosition (common scale)\nPosition (non-aligned scale)\nLength, Direction, Angle, Slope\nArea\nVolume, Density, Curvature\nShading, Color Saturation, Color Hue\n\nIf we compare a pie chart and a stacked bar chart, the bar chart asks readers to make judgements of position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not preferable – they make it harder on the reader, and as a result we are less accurate when reading information from pie charts.\nWhen creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc.\nThere is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area.\n\n\n\n\n\n\n\n\n\n\n.\n\nTed ED: How to spot a misleading graph - Lea Gaslowitz\nBusiness Insider: The Worst Graphs Ever\n\nExtra dimensions and other annotations are sometimes called “chartjunk” and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration).\nR graphics\n\nggplot2 cheat sheet\n\nggplot2 aesthetics cheat sheet - aesthetic mapping one page cheatsheet\nggplot2 reference guide\nggplot tricks\nR graph cookbook\nData Visualization in R\n\nCheck-in 2.2: Introduction to Data Visualization with ggplot2\nComplete the Canvas quiz to make sure you understand the basics of using ggplot2 to create graphics."
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#pa-2-using-data-visualization-to-find-the-penguins",
    "href": "02-tidy-data-and-basics-of-graphics.html#pa-2-using-data-visualization-to-find-the-penguins",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "PA 2: Using Data Visualization to Find the Penguins",
    "text": "PA 2: Using Data Visualization to Find the Penguins\nYou will be exploring different types of visualizations to uncover which species of penguins reside on different islands.\nVisit PA2: Using Data Visualization to Find the Penguins to see instructions for completing this activity."
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#footnotes",
    "href": "02-tidy-data-and-basics-of-graphics.html#footnotes",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "",
    "text": "though there are a seemingly infinite number of actual formats, and they pop up at the most inconvenient times↩︎\nThis isn’t necessarily a good thing, but you should know how to do it. The jury is still very much out on whether log transformations make data easier to read and understand↩︎\nWhen the COVID-19 outbreak started, many maps were using white-to-red gradients to show case counts and/or deaths. The emotional association between red and blood, danger, and death may have caused people to become more frightened than what was reasonable given the available information.↩︎\nLisa Charlotte Rost. What to consider when choosing colors for data visualization.↩︎\nSee this paper for the major source of this ranking; other follow-up studies have been integrated, but the essential order is largely unchanged.↩︎"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#ch3-objectives",
    "href": "03-data-cleaning-and-manipulation.html#ch3-objectives",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "Objectives",
    "text": "Objectives\n\nApply data manipulation verbs (filter, select, group by, summarize, mutate) to prepare data for analysis\nIdentify required sequence of steps for data cleaning\nDescribe step-by-step data cleaning process in lay terms appropriately and understand the consequences of data cleaning steps\nCreate summaries of data appropriate for analysis or display using data manipulation techniques"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#ch3-checkins",
    "href": "03-data-cleaning-and-manipulation.html#ch3-checkins",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere is one check-in for this week:\n\nCheck-in 3.1: Data Wrangling"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#a-quick-note-about-tibble",
    "href": "03-data-cleaning-and-manipulation.html#a-quick-note-about-tibble",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.1 A quick note about tibble\n",
    "text": "3.1 A quick note about tibble\n\nWe have been talking about our data in terms of data.frame objects in R. This is meant to inform you there is another object type in R called tibbles. Essentially, Tibbles are data frames, but they have certain features that make them easier to work with and provide additional cool features that can be useful (e.g., see nest()).\n\ntibble(\n  team   = c(\"A\", \"B\", \"C\", \"D\"), \n  points = c(22, 30, 18, 54)\n)\n\n# A tibble: 4 × 2\n  team  points\n  &lt;chr&gt;  &lt;dbl&gt;\n1 A         22\n2 B         30\n3 C         18\n4 D         54\n\n\nYou can use as_tibble() to convert data.frame objects in R to a tibble object.\nLearn more about tibbles\nYou can read more about Tibbles in R for Data Science: Tibbles"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#introduction-to-dplyr",
    "href": "03-data-cleaning-and-manipulation.html#introduction-to-dplyr",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.2 Introduction to dplyr\n",
    "text": "3.2 Introduction to dplyr\n\nIn this section, we’re going start learning how to work with data. Generally speaking, data doesn’t come in a form suitable for data visualization or statistical analysis1 - you have to clean it up, create the variables you care about, get rid of those you don’t care about, and so on.\nSome people call the process of cleaning and organizing your data “data wrangling”, which is a fantastic way to think about chasing down all of the issues in the data.\n\n\n\n\nData wrangling (by Allison Horst)\n\nWe will be using the tidyverse for this. It’s a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy2 and interface (basically, the commands will use predictable argument names and structure). You’ve already been introduced to parts of the tidyverse - specifically, readr and ggplot2.\ndplyr (one of the packages in the tidyverse) creates a “grammar of data manipulation” to make it easier to describe different operations. I find the dplyr grammar to be extremely useful when talking about data operations.\n\n\n\nEach dplyr verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, data comes first – literally, as it’s the first argument to any function. In addition, you don’t use df$variable to access a variable - you refer to the variable by its name alone (“bare” names). This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy.\nMain dplyr verbs\n\nfilter()\narrange()\nselect()\nmutate()\nsummarize()\nUse group_by() to perform group wise operations\nUse the pipe operator (|&gt; or %&gt;%) to chain together data wrangling operations\n\n\nThere is an excellent dplyr cheatsheet available from RStudio. You may want to print it out to have a copy to reference as you work through this chapter."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#motivation-example-dataset",
    "href": "03-data-cleaning-and-manipulation.html#motivation-example-dataset",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "Motivation & Example Dataset",
    "text": "Motivation & Example Dataset\nLast week we learned all about creating graphics in ggplot2. I am hoping to use data visualization as motivation going forward in this class – how do we get our data look like what we need in order to create the graph we want?\n\nLet’s explore how the dplyr verbs work, using the starwars data set, which contains a comprehensive list of the characters in the Star Wars movies and information about their height, mass, hair_color, skin_color, eye_color, birth_year, sex, gender, homeworld, species, films, vehicles, and starships.\nThis data set is included in the dplyr package, so we load that package and then use the data() function to load data set into memory. The loading isn’t complete until we actually use the data set though… so let’s look at our variables and types and print the first few rows.\n\nlibrary(dplyr)\ndata(starwars)\nstr(starwars)\nstarwars\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nLuke Skywalker\n172\n77\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nC-3PO\n167\n75\nNA\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nDroid\n\n\nR2-D2\n96\n32\nNA\nwhite, blue\nred\n33.0\nnone\nmasculine\nNaboo\nDroid\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nLeia Organa\n150\n49\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\nR5-D4\n97\n32\nNA\nwhite, red\nred\nNA\nnone\nmasculine\nTatooine\nDroid\n\n\nBiggs Darklighter\n183\n84\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\nObi-Wan Kenobi\n182\n77\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n\n\n\nWe could create a scatterplot of the character’s height by mass, color by species, and facet by homeworld.\n\nlibrary(ggplot2)\nggplot(data = starwars, aes(x = height, \n                            y = mass, \n                            color = species)\n       ) +\n  geom_point() +\n  facet_wrap(~ homeworld)\n\n\n\n\nThere is way too much going on in these plots to see anything of importance. Let’s break it down into the parts we are interested in."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#filter-pick-cases-rows-based-on-their-values",
    "href": "03-data-cleaning-and-manipulation.html#filter-pick-cases-rows-based-on-their-values",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.3 filter(): Pick cases (rows) based on their values",
    "text": "3.3 filter(): Pick cases (rows) based on their values\nFilter allows us to work with a subset of a larger data frame, keeping only the rows we’re interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from filter(). Note that unless we store the result from filter() in the original object, we don’t change the original.\n\n\n\n\n\ndplyr filter() by Allison Horst\n\nOnce the data is set up, filtering the data (selecting certain rows) is actually very simple. Of course, we’ve talked about how to use logical indexing before in Indexing Matrices, but here we’ll focus on using specific functions to perform the same operation.\nThe dplyr verb for selecting rows is filter(). filter() takes a set of one or more logical conditions, using bare column names and logical operators. Each provided condition is combined using AND.\n\nLet’s say we were interested in only the people, we could create a new data set starwars_people and filter on the species variable.\n\n# Get only the people\nstarwars_people &lt;- filter(.data = starwars, \n                          species == \"Human\"\n                          )\nstarwars_people\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nLuke Skywalker\n172\n77\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nLeia Organa\n150\n49\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\n\n\n\nWe can create the same plot with our new subset of data (starwars_people).\n\nCodeggplot(data = starwars_people, \n       mapping = aes(x = height, \n                     y = mass, \n                     color = species\n                     )\n       ) +\n  geom_point() +\n  facet_wrap(~ homeworld)\n\n\n\n\nThis looks better, but what if we only care about the people who come from Tatooine? Starting with our original starwars data set, we can combine logical AND statements with a comma to define a data subset called starwars_tatoonie_people.\n\n# Get only the people who come from Tatooine\nstarwars_tatooine_people &lt;- filter(.data = starwars, \n                                   species == \"Human\", \n                                   homeworld == \"Tatooine\"\n                                   )\nstarwars_tatooine_people\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nLuke Skywalker\n172\n77\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\nBiggs Darklighter\n183\n84\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\n\n\n\n\nCodeggplot(data = starwars_tatooine_people, \n       mapping = aes(x = height, \n                     y = mass, \n                     color = species\n                     )\n       ) +\n  geom_point() +\n  facet_wrap(~ homeworld)\n\n\n\n\n\n\nUseful comparison operations in R\nWe might not always want to only filter on a variable set equal to a certain category or value, the following operations can help you combine logical operations in filter().\n\n\n&gt; greater than\n\n&lt; less than\n\n== equal to\n\n%in% identifies if an element belongs to a vector\n\n| or\n\n3.3.1 Common Row Selection Tasks\nIn dplyr, there are a few helper functions which may be useful when constructing filter statements.\n\n\n\nFiltering by row number\nrow_number() is a helper function that is only used inside of another dplyr function (e.g. filter). You might want to keep only even rows, or only the first 10 rows in a table.\n\nNotice how we now have C-3PO, Darth Vader, Beru Whites, Anakin Skywalker, etc. (rows 2, 4, 6, …) from the original starwars data set output above.\n\nfilter(.data = starwars, \n       row_number() %% 2 == 0\n       ) \n\n\n\nEven Rows\nOriginal starwars\n\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nC-3PO\n167\n75\nNA\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nDroid\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nR5-D4\n97\n32\nNA\nwhite, red\nred\nNA\nnone\nmasculine\nTatooine\nDroid\n\n\nObi-Wan Kenobi\n182\n77\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n\n\n\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nLuke Skywalker\n172\n77\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nC-3PO\n167\n75\nNA\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nDroid\n\n\nR2-D2\n96\n32\nNA\nwhite, blue\nred\n33.0\nnone\nmasculine\nNaboo\nDroid\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nLeia Organa\n150\n49\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\nR5-D4\n97\n32\nNA\nwhite, red\nred\nNA\nnone\nmasculine\nTatooine\nDroid\n\n\nBiggs Darklighter\n183\n84\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\nObi-Wan Kenobi\n182\n77\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n\n\n\n\n\n\n\n\narrange() Sorting rows by variable values\nAnother common operation is to sort your data frame by the values of one or more variables.\narrange() is a dplyr verb for sorting rows in the table by one or more variables. It is often used with a helper function, desc(), which reverses the order of a variable, sorting it in descending order. Multiple arguments can be passed to arrange to sort the data frame by multiple columns hierarchically; each column can be modified with desc() separately.\n\nThe code below arranges the starwars characters tallest to shortest.\n\narrange(.data = starwars, \n        desc(height)\n        )\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nYarael Poof\n264\nNA\nnone\nwhite\nyellow\nNA\nmale\nmasculine\nQuermia\nQuermian\n\n\nTarfful\n234\n136\nbrown\nbrown\nblue\nNA\nmale\nmasculine\nKashyyyk\nWookiee\n\n\nLama Su\n229\n88\nnone\ngrey\nblack\nNA\nmale\nmasculine\nKamino\nKaminoan\n\n\nChewbacca\n228\n112\nbrown\nunknown\nblue\n200.0\nmale\nmasculine\nKashyyyk\nWookiee\n\n\nRoos Tarpals\n224\n82\nnone\ngrey\norange\nNA\nmale\nmasculine\nNaboo\nGungan\n\n\nGrievous\n216\n159\nnone\nbrown, white\ngreen, yellow\nNA\nmale\nmasculine\nKalee\nKaleesh\n\n\nTaun We\n213\nNA\nnone\ngrey\nblack\nNA\nfemale\nfeminine\nKamino\nKaminoan\n\n\nRugor Nass\n206\nNA\nnone\ngreen\norange\nNA\nmale\nmasculine\nNaboo\nGungan\n\n\nTion Medon\n206\n80\nnone\ngrey\nblack\nNA\nmale\nmasculine\nUtapau\nPau'an\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\n\n\n\n\nKeep the top \\(n\\) values of a variable\nslice_max() will keep the top values of a specified variable. This is like a filter statement, but it’s a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a few more lines of code.\n\nThe code below outputs the 5 tallest characters in star wars.\n\nslice_max(.data = starwars, \n          order_by = height, \n          n = 5\n          )\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nYarael Poof\n264\nNA\nnone\nwhite\nyellow\nNA\nmale\nmasculine\nQuermia\nQuermian\n\n\nTarfful\n234\n136\nbrown\nbrown\nblue\nNA\nmale\nmasculine\nKashyyyk\nWookiee\n\n\nLama Su\n229\n88\nnone\ngrey\nblack\nNA\nmale\nmasculine\nKamino\nKaminoan\n\n\nChewbacca\n228\n112\nbrown\nunknown\nblue\n200\nmale\nmasculine\nKashyyyk\nWookiee\n\n\nRoos Tarpals\n224\n82\nnone\ngrey\norange\nNA\nmale\nmasculine\nNaboo\nGungan\n\n\n\n\n\n\nOf course, there is a similar slice_min() function as well:\n\nThe code below outputs the 5 shortest characters in star wars.\n\nslice_min(.data = starwars, \n          order_by = height, \n          n = 5\n          )\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nYoda\n66\n17\nwhite\ngreen\nbrown\n896\nmale\nmasculine\nNA\nYoda's species\n\n\nRatts Tyerel\n79\n15\nnone\ngrey, blue\nunknown\nNA\nmale\nmasculine\nAleen Minor\nAleena\n\n\nWicket Systri Warrick\n88\n20\nbrown\nbrown\nbrown\n8\nmale\nmasculine\nEndor\nEwok\n\n\nDud Bolt\n94\n45\nnone\nblue, grey\nyellow\nNA\nmale\nmasculine\nVulpter\nVulptereen\n\n\nR2-D2\n96\n32\nNA\nwhite, blue\nred\n33\nnone\nmasculine\nNaboo\nDroid\n\n\nR4-P17\n96\nNA\nnone\nsilver, red\nred, blue\nNA\nnone\nfeminine\nNA\nDroid\n\n\n\n\n\nBy default, slice_max() and slice_min() return values tied with the nth value as well, which is why our result above has 6 rows.\n\n\n\nUse with_ties = FALSE.\n\nslice_min(.data = starwars, \n          order_by = height, \n          n = 5, \n          with_ties = FALSE\n          )\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nYoda\n66\n17\nwhite\ngreen\nbrown\n896\nmale\nmasculine\nNA\nYoda's species\n\n\nRatts Tyerel\n79\n15\nnone\ngrey, blue\nunknown\nNA\nmale\nmasculine\nAleen Minor\nAleena\n\n\nWicket Systri Warrick\n88\n20\nbrown\nbrown\nbrown\n8\nmale\nmasculine\nEndor\nEwok\n\n\nDud Bolt\n94\n45\nnone\nblue, grey\nyellow\nNA\nmale\nmasculine\nVulpter\nVulptereen\n\n\nR2-D2\n96\n32\nNA\nwhite, blue\nred\n33\nnone\nmasculine\nNaboo\nDroid\n\n\n\n\n\n\nslice_max and slice_min also take a prop argument that gives you a certain proportion of the values:\n\nThe code below outputs the shortest 1% of characters in star wars.\n\nslice_max(.data = starwars, \n          order_by = height, \n          prop = 0.01\n          )\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#select-pick-columns",
    "href": "03-data-cleaning-and-manipulation.html#select-pick-columns",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.4 select(): Pick columns",
    "text": "3.4 select(): Pick columns\nSometimes, we don’t want to work with a set of 50 variables when we’re only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g. df[, c(1, 3, 5)]), but this can get tedious.\nIn dplyr, the function to pick a few columns is select(). The syntax from the help file (?select) looks deceptively simple.\n\nselect(.data, …)\n\nSo as with just about every other tidyverse function, the first argument in a select statement is the data (.data =). After that, though, you can put just about anything that R can interpret. ... means something along the lines of “put in any additional arguments that make sense in context or might be passed on to other functions”.\nSo what can go in there?\n\n\nAn exhaustive(?) list of ways to select variables in dplyr\nFirst, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).3\nMost dplyr commands work with “bare” variable names - you don’t need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they’re very explicitly exceptions.\n\nvar3:var5: select(df, var3:var5) will give you a data frame with columns var3, anything between var3 and var 5, and var5\n\n!(&lt;set of variables&gt;) will give you any columns that aren’t in the set of variables in parentheses\n\n\n(&lt;set of vars 1&gt;) & (&lt;set of vars 2&gt;) will give you any variables that are in both set 1 and set 2. (&lt;set of vars 1&gt;) | (&lt;set of vars 2&gt;) will give you any variables that are in either set 1 or set 2.\n\nc() combines sets of variables.\n\n\n\ndplyr also defines a lot of variable selection “helpers” that can be used inside select() statements. These statements work with bare column names (so you don’t have to put quotes around the column names when you use them).\n\n\neverything() matches all variables\n\nlast_col() matches the last variable. last_col(offset = n) selects the n-th to last variable.\n\nstarts_with(\"xyz\") will match any columns with names that start with xyz. Similarly, ends_with() does exactly what you’d expect as well.\n\ncontains(\"xyz\") will match any columns with names containing the literal string “xyz”. Note, contains does not work with regular expressions (you don’t need to know what that means right now).\n\nmatches(regex) takes a regular expression as an argument and returns all columns matching that expression.\n\nnum_range(prefix, range) selects any columns that start with prefix and have numbers matching the provided numerical range.\n\nThere are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables.\n\n\nall_of(char) matches all variable names in the character vector char. If one of the variables doesn’t exist, this will return an error.\n\nany_of(char) matches the contents of the character vector char, but does not throw an error if the variable doesn’t exist in the data set.\n\nThere’s one final selector -\n\n\nwhere() applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative.\nLet’s try these selector functions out and see what we can accomplish!\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarting simple, let’s only subset and keep only the following variables from the starwars data set: name, height, mass, birth_year, species, and homeworld.\n\nselect(.data = starwars, name, height, mass, birth_year, species, homeworld)\n\n\n\n\n\nname\nheight\nmass\nbirth_year\nspecies\nhomeworld\n\n\n\nLuke Skywalker\n172\n77\n19.0\nHuman\nTatooine\n\n\nC-3PO\n167\n75\n112.0\nDroid\nTatooine\n\n\nR2-D2\n96\n32\n33.0\nDroid\nNaboo\n\n\nDarth Vader\n202\n136\n41.9\nHuman\nTatooine\n\n\nLeia Organa\n150\n49\n19.0\nHuman\nAlderaan\n\n\nOwen Lars\n178\n120\n52.0\nHuman\nTatooine\n\n\nBeru Whitesun Lars\n165\n75\n47.0\nHuman\nTatooine\n\n\nR5-D4\n97\n32\nNA\nDroid\nTatooine\n\n\nBiggs Darklighter\n183\n84\n24.0\nHuman\nTatooine\n\n\nObi-Wan Kenobi\n182\n77\n57.0\nHuman\nStewjon\n\n\n\n\n\nSince name, height, and mass are next to each other, we could have specified name:mass to tell us to select all of the columns between and including name to mass.\n\nselect(.data = starwars, name:mass, birth_year, species, homeworld)\n\n\n\n\n\nname\nheight\nmass\nbirth_year\nspecies\nhomeworld\n\n\n\nLuke Skywalker\n172\n77\n19.0\nHuman\nTatooine\n\n\nC-3PO\n167\n75\n112.0\nDroid\nTatooine\n\n\nR2-D2\n96\n32\n33.0\nDroid\nNaboo\n\n\nDarth Vader\n202\n136\n41.9\nHuman\nTatooine\n\n\nLeia Organa\n150\n49\n19.0\nHuman\nAlderaan\n\n\nOwen Lars\n178\n120\n52.0\nHuman\nTatooine\n\n\nBeru Whitesun Lars\n165\n75\n47.0\nHuman\nTatooine\n\n\nR5-D4\n97\n32\nNA\nDroid\nTatooine\n\n\nBiggs Darklighter\n183\n84\n24.0\nHuman\nTatooine\n\n\nObi-Wan Kenobi\n182\n77\n57.0\nHuman\nStewjon\n\n\n\n\n\n\nThe select column is also useful for reordering the variables in your data set.\n:::\nPerhaps we want the birth_year, sex, gender, homeworld, and species to follow the name of the star wars character. We can use the everything() function to specify we want all the other variables to follow.\n\nselect(.data = starwars, name, birth_year:species, everything())\n\n\n\n\n\nname\nbirth_year\nsex\ngender\nhomeworld\nspecies\nheight\nmass\nhair_color\nskin_color\neye_color\n\n\n\nLuke Skywalker\n19.0\nmale\nmasculine\nTatooine\nHuman\n172\n77\nblond\nfair\nblue\n\n\nC-3PO\n112.0\nnone\nmasculine\nTatooine\nDroid\n167\n75\nNA\ngold\nyellow\n\n\nR2-D2\n33.0\nnone\nmasculine\nNaboo\nDroid\n96\n32\nNA\nwhite, blue\nred\n\n\nDarth Vader\n41.9\nmale\nmasculine\nTatooine\nHuman\n202\n136\nnone\nwhite\nyellow\n\n\nLeia Organa\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n150\n49\nbrown\nlight\nbrown\n\n\nOwen Lars\n52.0\nmale\nmasculine\nTatooine\nHuman\n178\n120\nbrown, grey\nlight\nblue\n\n\nBeru Whitesun Lars\n47.0\nfemale\nfeminine\nTatooine\nHuman\n165\n75\nbrown\nlight\nblue\n\n\nR5-D4\nNA\nnone\nmasculine\nTatooine\nDroid\n97\n32\nNA\nwhite, red\nred\n\n\nBiggs Darklighter\n24.0\nmale\nmasculine\nTatooine\nHuman\n183\n84\nblack\nlight\nbrown\n\n\nObi-Wan Kenobi\n57.0\nmale\nmasculine\nStewjon\nHuman\n182\n77\nauburn, white\nfair\nblue-gray\n\n\n\n\n\nNote that everything() won’t duplicate columns you’ve already added.\n:::\nSo for now, at least in R, you know how to cut your data down to size rowwise (with filter) and column-wise (with select).\n\ndplyr::relocate\nAnother handy dplyr function is relocate; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I’m covering relocate here mostly because it also comes with this amazing cartoon illustration.\n\n\nrelocate lets you rearrange columns (by Allison Horst)\n\n\n# move numeric variables to the front\nrelocate(.data = starwars, \n         where(is.numeric)\n         )\n\n# A tibble: 87 × 11\n   height  mass birth_year name     hair_color skin_color eye_color sex   gender\n    &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; \n 1    172    77       19   Luke Sk… blond      fair       blue      male  mascu…\n 2    167    75      112   C-3PO    &lt;NA&gt;       gold       yellow    none  mascu…\n 3     96    32       33   R2-D2    &lt;NA&gt;       white, bl… red       none  mascu…\n 4    202   136       41.9 Darth V… none       white      yellow    male  mascu…\n 5    150    49       19   Leia Or… brown      light      brown     fema… femin…\n 6    178   120       52   Owen La… brown, gr… light      blue      male  mascu…\n 7    165    75       47   Beru Wh… brown      light      blue      fema… femin…\n 8     97    32       NA   R5-D4    &lt;NA&gt;       white, red red       none  mascu…\n 9    183    84       24   Biggs D… black      light      brown     male  mascu…\n10    182    77       57   Obi-Wan… auburn, w… fair       blue-gray male  mascu…\n# ℹ 77 more rows\n# ℹ 2 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#mutate-add-and-transform-variables",
    "href": "03-data-cleaning-and-manipulation.html#mutate-add-and-transform-variables",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.5 mutate(): Add and transform variables",
    "text": "3.5 mutate(): Add and transform variables\nUp to this point, we’ve been primarily focusing on how to decrease the dimensionality of our data set in various ways (i.e., remove rows or columns from the original data set). But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is mutate().\n\n\n\n\n\nMutate (by Allison Horst)\n\n\nLet’s create a new variable, BMI calculated from existing columns – mass/height\\(^2\\)\n\nmutate(.data = starwars, \n       BMI = mass/height^2, \n       .after = mass\n       )\n\n\n\n\n\nname\nheight\nmass\nBMI\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nLuke Skywalker\n172\n77\n0.0026028\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nC-3PO\n167\n75\n0.0026892\nNA\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nDroid\n\n\nR2-D2\n96\n32\n0.0034722\nNA\nwhite, blue\nred\n33.0\nnone\nmasculine\nNaboo\nDroid\n\n\nDarth Vader\n202\n136\n0.0033330\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nLeia Organa\n150\n49\n0.0021778\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\nOwen Lars\n178\n120\n0.0037874\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\n0.0027548\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\nR5-D4\n97\n32\n0.0034010\nNA\nwhite, red\nred\nNA\nnone\nmasculine\nTatooine\nDroid\n\n\nBiggs Darklighter\n183\n84\n0.0025083\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\nObi-Wan Kenobi\n182\n77\n0.0023246\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n\n\n\nBy default, the new variable will be tacked on to the end of the data set as the last column. Using .after or .before arguments allows you to place the new variable in the middle of the data set.\n\nWe can combine the mutate function with other variables such as ifelse().\n\nLet’s replace the species variable to indicate Human or Not Human.\n\nmutate(.data = starwars, \n       species = if_else(species == \"Human\", \n                        species, \n                        \"Not Human\"\n                        )\n       )\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nLuke Skywalker\n172\n77\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nC-3PO\n167\n75\nNA\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nNot Human\n\n\nR2-D2\n96\n32\nNA\nwhite, blue\nred\n33.0\nnone\nmasculine\nNaboo\nNot Human\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nLeia Organa\n150\n49\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\nR5-D4\n97\n32\nNA\nwhite, red\nred\nNA\nnone\nmasculine\nTatooine\nNot Human\n\n\nBiggs Darklighter\n183\n84\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\nObi-Wan Kenobi\n182\n77\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n\n\n\n\nThe learning curve here isn’t actually knowing how to assign new variables (though that’s important). The challenge comes when you want to do something new and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables. We will cover special data types like these in a few weeks!\n\nMutate and new challenges\nI’m not going to be able to teach you how to handle every mutate statement task you’ll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to read documentation and Google things intelligently, and to understand what you’re reading enough to actually implement it. This is something that comes with practice (and lots of Googling, stack overflow searches, etc.).\nGoogle and StackOverflow are very common and important programming skills!\n\n\nSource\n\n\n\nSource\n\nIn this textbook, the examples will expose you to solutions to common problems (or require that you do some basic reading yourself); unfortunately, there are too many common problems for us to work through line-by-line.\nPart of the goal of this textbook is to help you learn how to read through a package description and evaluate whether the package will do what you want. We’re going to try to build some of those skills starting now. It would be relatively easy to teach you how to do a set list of tasks, but you’ll be better statisticians and programmers if you learn the skills to solve niche problems on your own.\n\n\nApologies for the noninclusive language, but the sentiment is real. Source"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#summarize",
    "href": "03-data-cleaning-and-manipulation.html#summarize",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.6 summarize()\n",
    "text": "3.6 summarize()\n\nThe next verb is one that we’ve already implicitly seen in action: summarize() takes a data frame with potentially many rows of data and reduces it down to one row of data using some function.\n\n\n\n\nHere (in a trivial example), I compute the overall average height of a star war’s character.\n\nsummarize(.data = starwars,\n          avg_height = mean(height, na.rm = T)\n          )\n\n# A tibble: 1 × 1\n  avg_height\n       &lt;dbl&gt;\n1       175.\n\n\nThe na.rm = T argument says to ignore/remove the missing (NA) values in calculating the average.\n\nThe real power of summarize, though, is in combination with group_by. We’ll see more summarize examples, but it’s easier to make good examples when you have all the tools - it’s hard to demonstrate how to use a hammer if you don’t also have a nail."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#group_by-group-by-power",
    "href": "03-data-cleaning-and-manipulation.html#group_by-group-by-power",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.7 group_by() Group By + (?) = Power!",
    "text": "3.7 group_by() Group By + (?) = Power!\nFrequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to\n\n\n\n\nsplit my data set into smaller data sets - one for each day\ncompute summary values for each smaller data set\nput my summarized data back together into a single data set\n\nThis is known as the split-apply-combine “Group by: Split-Apply-Combine” (2022) or sometimes, map-reduce (Dean and Ghemawat 2008) strategy (though map-reduce is usually on specifically large data sets and performed in parallel).\nIn tidy parlance, group_by() is the verb that accomplishes the first task. summarize() accomplishes the second task and implicitly accomplishes the third as well.\n\nLet’s see how things change when we calculate the average height of star wars characters by their species.\n\nstarwars |&gt; \n  group_by(species) |&gt; \n  summarize(height = mean(height, na.rm = T))\n\n# A tibble: 38 × 2\n   species   height\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Aleena       79 \n 2 Besalisk    198 \n 3 Cerean      198 \n 4 Chagrian    196 \n 5 Clawdite    168 \n 6 Droid       131.\n 7 Dug         112 \n 8 Ewok         88 \n 9 Geonosian   183 \n10 Gungan      209.\n# ℹ 28 more rows\n\n\nThe next section Pipe Operator will introduce and talk about what the |&gt; symbol is, this example is just hard to show without it!\n\n\n\n\n\n\nThe ungroup() command is just as important as the group_by() command! (by Allison Horst)\n\nWhen you group_by() a variable, your result carries this grouping with it. summarize() will remove one layer of grouping (by default), but if you ever want to return to a completely ungrouped data set, you should use the ungroup() command."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#pipe",
    "href": "03-data-cleaning-and-manipulation.html#pipe",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.8 Pipe Operator",
    "text": "3.8 Pipe Operator\nThe powerhouse of the tidyverse package comes from the pipe operator. This specifies a sequence of operations (kind of like how we layered our graphics in ggplot2). The output from the previous line (often a subset) is automatically passed into the first argument of the next line (remember, data first! .data =).\nThe native pipe operator is |&gt;, but the magrittr pipe operator %&gt;% was used up until recently (and still is often used!).\n\n\n\n\n\n\nThe keyboard shortcut for adding a pipe operator to your code is Ctrl/Cmd + Shift + M.\nHowever, if you want to use this shortcut for the native pipe, you need to change your global R settings:\nTools &gt; Global Options &gt; Code &gt; checkbox Use native pipe operator, |&gt;\n\n\n\n(required) Read more about the pipe operators at Workflow Pipes.\n\n\n\nLet’s combine all of our new skills with the pipe operator!\n\n\nUse filter() to subset our data to only Human’s and Droid’s\nUse mutate() to create the new variable, BMI,\nUse group_by() to create groups by species,\nUse summarize() to calculate the mean and standard deviation of BMI\n\nUse mutate() to calculate the average plus/minus one standard deviation.\n\nWe could either assign this new data set that has summary values of BMI by species or we could pipe the data set directly into a plot – recall the first argument for ggplot() is data =.\n\nstarwars |&gt; \n  filter(species %in% c(\"Human\", \"Droid\")) |&gt; \n  mutate(BMI = mass/height^2) |&gt; \n  group_by(species) |&gt; \n  summarize(avg_BMI = mean(BMI, na.rm = TRUE),\n            sd_BMI = sd(BMI, na.rm = TRUE)\n            ) |&gt; \n  mutate(BMI_1sd_below = avg_BMI - sd_BMI,\n         BMI_1sd_above = avg_BMI + sd_BMI\n         ) |&gt; \n  ggplot(aes(x = species, \n             y = avg_BMI)\n         ) +\n  geom_point() +\n  geom_errorbar(aes(ymin = BMI_1sd_below,\n                    ymax = BMI_1sd_above),\n                width = 0.2\n                ) +\n  labs(x = \"Species\", \n       subtitle = \"Average BMI\") +\n  theme(axis.title.y = element_blank())\n\n\n\n\nAs with ggplot, formatting your dplyr code pipelines so it is readable will help both you and me!"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#checkin3-1",
    "href": "03-data-cleaning-and-manipulation.html#checkin3-1",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "Check-in 3.1: Data Wrangling",
    "text": "Check-in 3.1: Data Wrangling\nQ1: Arrange the pipeline\nWorking with the Palmer Penguins data set:\n\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nSuppose we would like to study how the ratio of penguin body mass to flipper size differs across the species. Arrange the following steps into an order that accomplishes this goal (assuming the steps are connected with a |&gt; or a %&gt;%).\narrange(med_mass_flipper_ratio)\ngroup_by(species)\npenguins\nsummarize(med_mass_flipper_ratio = median(mass_flipper_ratio))\nmutate(mass_flipper_ratio = body_mass_g / flipper_length_mm)\n\nQ2 - Q7: dplyr pipelines\nConsider the base R code below.\n\nmean(penguins[penguins$species == \"Adelie\", ]$body_mass_g, na.rm = T)\n\n[1] 3700.662\n\n\nFor each of the following dplyr pipelines, indicate which of the following is true:\n\nIt returns the exact same thing as the (above) base R code\nIt returns the correct information, but the wrong object type\nIt returns incorrect information\nIt returns an error\n\nQ2\n\npenguins  |&gt;\n  filter(\"body_mass_g\") |&gt;\n  pull(\"Adelie\") |&gt;\n  mean(na.rm = T)\n\nQ3\n\npenguins |&gt;\n  filter(species == \"Adelie\") |&gt;\n  select(body_mass_g) |&gt;\n  summarize(mean(body_mass_g, na.rm = T))\n\nQ4\n\npenguins |&gt;\n  pull(body_mass_g) |&gt;\n  filter(species == \"Adelie\") |&gt;\n  mean(na.rm = T)\n\nQ5\n\npenguins |&gt;\n  filter(species == \"Adelie\") |&gt;\n  select(body_mass_g) |&gt;\n  mean(na.rm = T)\n\nQ6\n\npenguins |&gt;\n  filter(species == \"Adelie\") |&gt;\n  pull(body_mass_g) |&gt;\n  mean(na.rm = T)\n\nQ7\n\npenguins |&gt;\n  select(species == \"Adelie\") |&gt;\n  filter(body_mass_g) |&gt;\n  summarize(mean(body_mass_g, na.rm = T))"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#additional-resources",
    "href": "03-data-cleaning-and-manipulation.html#additional-resources",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nIntroduction to dplyr and Single Table dplyr functions\nR for Data Science: Data Transformations\nModern Dive: Data Wrangling\nAdditional practice exercises: Intro to the tidyverse, group_by + summarize examples, group_by + mutate examples (from a similar class at Iowa State)\nVideos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of dplyr as well."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#pa-3-identify-the-mystery-college",
    "href": "03-data-cleaning-and-manipulation.html#pa-3-identify-the-mystery-college",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "PA 3: Identify the Mystery College",
    "text": "PA 3: Identify the Mystery College\nToday you will be using the dplyr package to clean a data set and then using that cleaned data set to figure out what college Ephelia has been accepted to.\nVisit PA 3: Identify the Mystery College for instructions.\nSubmit the full name of the college Ephelia will attend to the Canvas Quiz."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#references",
    "href": "03-data-cleaning-and-manipulation.html#references",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "References",
    "text": "References\n\n\n\n\nDean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified Data Processing on Large Clusters.” Communications of the ACM 51 (1): 107–13. https://doi.org/10.1145/1327452.1327492.\n\n\n“Group by: Split-Apply-Combine.” 2022. In Pandas 1.4.3 Documentation. Python. https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40: 1–29."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#footnotes",
    "href": "03-data-cleaning-and-manipulation.html#footnotes",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "",
    "text": "See this twitter thread for some horror stories. This tweet is also pretty good at showing one type of messiness.↩︎\nThe philosophy includes a preference for pipes, but this preference stems from the belief that code should be readable in the same way that text is readable.↩︎\nIt accomplishes this through the magic of quasiquotation, which we will not cover in this course because it’s basically witchcraft.↩︎"
  },
  {
    "objectID": "04-data-joins-and-transformations.html#ch4-objectives",
    "href": "04-data-joins-and-transformations.html#ch4-objectives",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "Objectives",
    "text": "Objectives\nThe first section of this chapter is heavily outsourced to r4ds as they do a much better job at providing examples and covering the extensive functionality of factor data types than I myself would ever be able to.\n\nUse forcats to reorder and relabel factor variables in data cleaning steps and data visualizations.\n\nBroadly, your objective while reading the second section of this chapter is to be able to identify data sets which have “messy” formats and determine a sequence of operations to transition the data into “tidy” format. To do this, you should master the following concepts:\n\nDetermine what data format is necessary to generate a desired plot or statistical model.\nUnderstand the differences between “wide” and “long” format data and how to transition between the two structures.\nUnderstand relational data formats and how to use data joins to assemble data from multiple tables into a single table.\n\n\nFunctions covered this week:\nlibrary(forcats) (not an exhaustive list)\n\n\nfactor(), levels()\n\nfct_relevel()\n\nfct_reorder() fct_reorder2()\n\nfct_collapse()\n\nDownload the forcats cheatsheet.\nlibrary(tidyr)\n\n\npivot_longer(), pivot_wider()\n\n\nseparate(), unite()\n\n\nlibrary(dplyr)\n\n\nleft_join(), right_join(), full_join()\n\n\nsemi_join(), anti_join()"
  },
  {
    "objectID": "04-data-joins-and-transformations.html#ch4-checkins",
    "href": "04-data-joins-and-transformations.html#ch4-checkins",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere are three check-ins for this week:\n\nCheck-in 4.1: Functions from forcats\nCheck-in 4.2: Practice with Pivoting and Joins"
  },
  {
    "objectID": "04-data-joins-and-transformations.html#factors-with-forcats",
    "href": "04-data-joins-and-transformations.html#factors-with-forcats",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "\n4.1 Factors with forcats\n",
    "text": "4.1 Factors with forcats\n\nWe have been floating around the idea of factor data types. In this section, we will formally define factors and why they are needed for data visualization and analysis. We will then learn useful functions for working with factors in our data cleaning steps.\n\n\n\nIn short, factors are categorical variables with a fixed number of values (think a set number of groups). One of the main features that set factors apart from groups is that you can reorder the groups to be non-alphabetical. In this section we will be using the forcats package (part of the tidyverse!) to create and manipulate factor variables.\n\n(Required) Go read about factors in r4ds."
  },
  {
    "objectID": "04-data-joins-and-transformations.html#checkin4-1",
    "href": "04-data-joins-and-transformations.html#checkin4-1",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "Check-in 4.1: Functions from forcats\n",
    "text": "Check-in 4.1: Functions from forcats\n\nAnswer the following questions in the Canvas Quiz.\n1. Which of the following tasks can fct_recode() accomplish?\n\nchanges the values of the factor levels\nreorders the levels of a factor\nremove levels of a factor you don’t want\ncollapse levels of a factor into a new level\n\n2. Which of the following tasks can fct_relevel() accomplish?\n\nreorders the levels of a factor\nchanges the values of the factor levels\nremove levels of a factor you don’t want\ncollapse levels of a factor into a new level\n\n3. What is the main difference between fct_collapse() and fct_recode()?\n\n\nfct_recode() uses strings to create factor levels\n\nfct_recode() uses groups to create factor levels\n\nfct_recode() cannot create an “Other” group\n\n4. What ordering do you get with fct_reorder()?\n\nlargest to smallest based on another variable\norder of appearance\nlargest to smallest based on counts\nalphabetical order\n\n5. What ordering do you get with fct_inorder()?\n\norder of appearance\nalphabetical order\nlargest to smallest based on counts\nlargest to smallest based on another variable"
  },
  {
    "objectID": "04-data-joins-and-transformations.html#identifying-the-problem-messy-data",
    "href": "04-data-joins-and-transformations.html#identifying-the-problem-messy-data",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "\n4.2 Identifying the problem: Messy data",
    "text": "4.2 Identifying the problem: Messy data\nThe illustrations below are lifted from an excellent blog post (Lowndes and Horst 2020) about tidy data; they’re reproduced here because\n\n\n\n\nthey’re beautiful and licensed as CCA-4.0-by, and\nthey might be more memorable than the equivalent paragraphs of text without illustration.\n\nMost of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data entry or viewing, rather than for statistical analysis:\n\nTables may be laid out for easy data entry, so that there are multiple observations in a single row\nIt may be visually preferable to arrange columns of data to show multiple times or categories on the same row for easy comparison\n\nWhen we analyze data, however, we care much more about the fundamental structure of observations: discrete units of data collection. Each observation may have several corresponding variables that may be measured simultaneously, but fundamentally each discrete data point is what we are interested in analyzing or plotting.\nThe structure of tidy data reflects this preference for keeping the data in a fundamental form: each observation is in its own row, any observed variables are in single columns. This format is inherently rectangular, which is also important for statistical analysis - our methods are typically designed to work with matrices of data.\n\n\nFigure 4.1: Tidy data format, illustrated.\n\n\n\nAn illustration of the principle that every messy dataset is messy in its own way.\n\nThe preference for tidy data has several practical implications: it is easier to reuse code on tidy data, allowing for analysis using a standardized set of tools (rather than having to build a custom tool for each data analysis job).\n\n\nTidy data is easier to manage because the same tools and approaches apply to multiple datasets.\n\nIn addition, standardized tools for data analysis means that it is easier to collaborate with others: if everyone starts with the same set of assumptions about the dataset, you can borrow methods and tools from a collaborator’s analysis and easily apply them to your own dataset.\n\n\n\n\n\nCollaboration with tidy data.\n\n\n\n\n\nTidy data enables standardized workflows.\n\n\n\nFigure 4.2: Tidy data makes it easier to collaborate with others and analyze new data using standardized workflows.\n\n\n\nExamples: Messy Data\nThese datasets all display the same data: TB (Tuberculosis) cases documented by the WHO (World Health Organization) in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.\n\nFor each of the data set, determine whether each table is tidy. If it is not, identify which rule or rules it violates.\nWhat would you have to do in order to compute a standardized TB infection rate per 100,000 people?\n\nAll of these data sets are “built-in” to the tidyverse package\n\n\nTable 1\n2\n3\n4\n5\n\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nHere, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column).\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nHere, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nThis form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can’t do anything with this format as it stands, because we can’t do math on data stored as characters. However, this form might be easier to read and record for a human being.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nIn this form, we have two tables - one for population, and one for cases. Each year’s observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we’ll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nTable 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don’t always handle dates the way you’d hope they would.\n\n\n\n:::\n\nBy the end of this chapter, you will have the skills needed to wrangle and transform the most common “messy” data sets into “tidy” form."
  },
  {
    "objectID": "04-data-joins-and-transformations.html#pivot-operations",
    "href": "04-data-joins-and-transformations.html#pivot-operations",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "\n4.3 Pivot Operations",
    "text": "4.3 Pivot Operations\nIt’s fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren’t necessarily the most friendly for analysis.\n\nThe two operations we’ll learn here are wide -&gt; long and long -&gt; wide.\n\nThis animation uses the functions pivot_wider() and pivot_longer() from the tidyr package in R – Animation source.\n\n4.3.1 Longer\nIn many cases, the data come in what we might call “wide” form - some of the column names are not names of variables, but instead, are themselves values of another variable.\n\n\n\n\n\nPicture the Operation\npivot_longer()\n\n\n\nTables 4a and 4b (from above) are good examples of data which is in “wide” form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively.\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\nThe solution to this is to rearrange the data into “long form”: to take the columns which contain values and “stack” them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn’t being stacked (e.g. country, in both the example above and the image below).\n\n\nA visual representation of what the pivot_longer operation looks like in practice.\n\nOnce our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we’ll have tidy(er) data.\n\n\n\ntable4a |&gt; \n  pivot_longer(cols = `1999`:`2000`, \n               names_to = \"year\", \n               values_to = \"cases\"\n               )\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\ntable4b |&gt; \n  pivot_longer(cols = -country, \n               names_to = \"year\", \n               values_to = \"population\"\n               )\n\n# A tibble: 6 × 3\n  country     year  population\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\nThe columns are moved to a variable with the name passed to the argument “names_to” (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument “values_to” (again, hopefully easy to remember).\nWe identify ID variables (variables which we don’t want to pivot) by not including them in the pivot statement. We can do this in one of two ways:\n\nselect only variables (columns) we want to pivot (see table4a pivot)\n\nselect variables (columns) we don’t want to pivot, using - to remove them (see table4b pivot)\n\n\nWhich option is easier depends how many things you’re pivoting (and how the columns are structured).\n\n\n\n\n4.3.2 Wider\nWhile it’s very common to need to transform data into a longer format, it’s not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again.\n\n\n\n\n\nPicture the Operation\npivot_wider()\n\n\n\nTable 2 (from above) is an example of a table that is in long format but needs to be converted to a wider layout to be “tidy” - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows.\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\nA visual representation of what the pivot_wider operation looks like in practice.\n\n\n\n\ntable2 |&gt;\n  pivot_wider(names_from  = type, \n              values_from = count\n              )\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n\n\nLearn More in R4DS\nRead more about pivoting in r4ds."
  },
  {
    "objectID": "04-data-joins-and-transformations.html#separating-and-uniting-variables",
    "href": "04-data-joins-and-transformations.html#separating-and-uniting-variables",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "\n4.4 Separating and Uniting Variables",
    "text": "4.4 Separating and Uniting Variables\nWe will talk about strings and regular expressions next week, but there’s a task that is fairly commonly encountered with functions that belong to the tidyr package: separating variables into two different columns separate() and it’s complement, unite(), which is useful for combining two variables into one column.\n\n\nseparate_wider_delim() and separate_wider_position()\nunite()\n\n\n\n\n\n\n\nA visual representation of what separating variables means for data set operations.\n\n\n\n\ntable3 |&gt;\n  separate_wider_delim(cols        = rate,\n                       names       = c(\"cases\", \"population\"),\n                       delim       = \"/\",\n                       cols_remove = FALSE\n           )\n\n# A tibble: 6 × 5\n  country      year cases  population rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;            \n1 Afghanistan  1999 745    19987071   745/19987071     \n2 Afghanistan  2000 2666   20595360   2666/20595360    \n3 Brazil       1999 37737  172006362  37737/172006362  \n4 Brazil       2000 80488  174504898  80488/174504898  \n5 China        1999 212258 1272915272 212258/1272915272\n6 China        2000 213766 1280428583 213766/1280428583\n\n\nI’ve left the rate column in the original data frame (cols_remove = F) just to make it easy to compare and verify that yes, it worked.\n\n\nAnd, of course, there is a complementary operation, which is when it’s necessary to join two columns to get a useable data value.\n\n\n\n\nA visual representation of what uniting variables means for data set operations.\n\n\n\n\ntable5 |&gt;\n  unite(col = \"year\",\n        c(century, year),\n        sep = ''\n        )\n\n# A tibble: 6 × 3\n  country     year  rate             \n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;            \n1 Afghanistan 1999  745/19987071     \n2 Afghanistan 2000  2666/20595360    \n3 Brazil      1999  37737/172006362  \n4 Brazil      2000  80488/174504898  \n5 China       1999  212258/1272915272\n6 China       2000  213766/1280428583\n\n\n\n\n\n\nLearn More in R4DS\nThe separate_xxx() is actually a family of experimental functions stemming from the superseeded separate() function. You can read more about separate_xxx() and unite() in r4ds and r4ds."
  },
  {
    "objectID": "04-data-joins-and-transformations.html#merging-tables",
    "href": "04-data-joins-and-transformations.html#merging-tables",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "\n4.5 Merging Tables",
    "text": "4.5 Merging Tables\nThe final essential data tidying and transformation skill you need to acquire is joining tables. It is common for data to be organized relationally - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between the individual data points and the groups of data points that have to be documented.\n\nExamples: Relational Data Example: Primary School Records\nEach individual has certain characteristics:\n\nfull_name\ngender\nbirth date\nID number\n\nEach student has specific characteristics:\n\nID number\nparent name\nparent phone number\nmedical information\nClass ID\n\nTeachers may also have additional information:\n\nID number\nClass ID\nemployment start date\neducation level\ncompensation level\n\nThere are also fields like grades, which occur for each student in each class, but multiple times a year.\n\nID number\nStudent ID\nClass ID\nyear\nterm number\nsubject\ngrade\ncomment\n\nAnd for teachers, there are employment records on a yearly basis\n\nID number\nEmployee ID\nyear\nrating\ncomment\n\nBut each class also has characteristics that describe the whole class as a unit:\n\nlocation ID\nclass ID\nmeeting time\ngrade level\n\nEach location might also have some logistical information attached:\n\nlocation ID\nroom number\nbuilding\nnumber of seats\nAV equipment\n\n\nWe could go on, but you can see that this data is hierarchical, but also relational: - each class has both a teacher and a set of students - each class is held in a specific location that has certain equipment\nIt would be silly to store this information in a single table (though it probably can be done) because all of the teacher information would be duplicated for each student in each class; all of the student’s individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well.\nBut, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps:\n\nget the Class ID\nget any teachers that are assigned that Class ID - specifically, get their ID number\nget any students that are assigned that Class ID - specifically, get their ID number\nappend the results from teachers and students so that there is a list of all individuals in the class\nlook through the “individual data” table to find any individuals with matching ID numbers, and keep those individuals’ birth days.\n\nIt is helpful to develop the ability to lay out a set of tables in a schema (because often, database schemas aren’t well documented) and mentally map out the steps that you need to combine tables to get the information you want from the information you have.\nTable joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information.\nkeys are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A primary key identifies an observation in its own table. A foreign key identifies an observation in another table.\nThere are 3 main types of table joins:\n\nMutating joins, which add columns from one table to matching rows in another table\nEx: adding birthday to the table of all individuals in a class\nFiltering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don’t change)\nEx: finding all teachers or students who have class ClassID\nSet operations, which treat observations as set elements (e.g. union, intersection, etc.)\nEx: taking the union of all student and teacher IDs to get a list of individual IDs\n\n\n4.5.1 Animating Joins\nNote: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.\nIf we start with two tables, x and y,\n\nMutating Joins\nWe’re primarily going to focus on mutating joins, as filtering joins can be accomplished by … filtering … rather than by table joins.\n\n\nInner Join\nLeft Join\nRight Join\nFull Join\n\n\n\nWe can do a filtering inner_join to keep only rows which are in both tables (but we keep all columns)\n\n\n\nBut what if we want to keep all of the rows in x? We would do a left_join\n\nIf there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one.\n\n\n\nIf we wanted to keep all of the rows in y, we would do a right_join:\n\n(or, we could do a left join with y and x, but… either way is fine).\n\n\nAnd finally, if we want to keep all of the rows, we’d do a full_join:\n\nYou can find other animations corresponding to filtering joins and set operations here\n\n\n\nEvery join has a “left side” and a “right side” - so in some_join(A, B), A is the left side, B is the right side.\nJoins are differentiated based on how they treat the rows and columns of each side. In mutating joins, the columns from both sides are always kept.\n\n\n\n\n\n\n\n\n\n\nLeft Side\nRight Side\n\n\n\n\nJoin Type\nRows\nCols\n\n\ninner\nmatching\nall\nmatching\n\n\nleft\nall\nall\nmatching\n\n\nright\nmatching\nall\nall\n\n\nouter\nall\nall\nall\n\n\n\n\nDemonstration: Mutating Joins\n\nt1 &lt;- tibble(x = c(\"A\", \"B\", \"D\"), \n             y = c(1, 2, 3)\n             )\nt1\n\n# A tibble: 3 × 2\n  x         y\n  &lt;chr&gt; &lt;dbl&gt;\n1 A         1\n2 B         2\n3 D         3\n\nt2 &lt;- tibble(x = c(\"B\", \"C\", \"D\"), \n             z = c(2, 4, 5)\n             )\nt2\n\n# A tibble: 3 × 2\n  x         z\n  &lt;chr&gt; &lt;dbl&gt;\n1 B         2\n2 C         4\n3 D         5\n\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\ninner_join(t1, t2)\n\n# A tibble: 2 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 B         2     2\n2 D         3     5\n\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs.\n\nleft_join(t1, t2)\n\n# A tibble: 3 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1    NA\n2 B         2     2\n3 D         3     5\n\nleft_join(t2, t1)\n\n# A tibble: 3 × 3\n  x         z     y\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 B         2     2\n2 C         4    NA\n3 D         5     3\n\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\nright_join(t1, t2)\n\n# A tibble: 3 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 B         2     2\n2 D         3     5\n3 C        NA     4\n\nright_join(t2, t1)\n\n# A tibble: 3 × 3\n  x         z     y\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 B         2     2\n2 D         5     3\n3 A        NA     1\n\n\nAn outer join keeps everything - all rows, all columns. In dplyr, it’s known as a full_join.\n\nfull_join(t1, t2)\n\n# A tibble: 4 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1    NA\n2 B         2     2\n3 D         3     5\n4 C        NA     4\n\n\n\nFiltering Joins\n\n\nSemi Join\nAnti Join\n\n\n\nA semi join keeps matching rows from x and y, discarding all other rows and keeping only the columns from x.\n\n\n\nAn anti-join keeps rows in x that do not have a match in y, and only keeps columns in x.\n\n\n\n\n\nLearn More in R4DS\nRead more about joins in r4ds"
  },
  {
    "objectID": "04-data-joins-and-transformations.html#checkin4-2",
    "href": "04-data-joins-and-transformations.html#checkin4-2",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "Check-in 4.2: Practice with Pivoting & Joins",
    "text": "Check-in 4.2: Practice with Pivoting & Joins\nComplete the code templates in the Canvas Quiz to complete these tasks.\nPart 1 (Pivoting) Load in the cereal data set Note: you may need to install.packages(\"liver\"):\n\n\n\n\nname\nmanuf\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n100% Bran\nN\ncold\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1\n0.33\n68.40297\n\n\n100% Natural Bran\nQ\ncold\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1\n1.00\n33.98368\n\n\nAll-Bran\nK\ncold\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1\n0.33\n59.42551\n\n\nAll-Bran with Extra Fiber\nK\ncold\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1\n0.50\n93.70491\n\n\nAlmond Delight\nR\ncold\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1\n0.75\n34.38484\n\n\nApple Cinnamon Cheerios\nG\ncold\n110\n2\n2\n180\n1.5\n10.5\n10\n70\n25\n1\n1\n0.75\n29.50954\n\n\n\n\n\n\nlibrary(liver)\ndata(cereal)\nhead(cereal)\n\nQuestion 1 Create a new data set called cereals_long, that has three columns:\n\nThe name of the cereal\nA column called Nutrient with values “protein”, “fat”, or “fiber”.\nA column called Amount with the corresponding amount of the nutrient.\n\nPart 2 (Joins) The following code creates three data sets:\n\nprof_info &lt;- data.frame(\n  professor        = c(\"Bodwin\", \"Glanz\", \"Carlton\", \"Sun\", \"Robinson\"),\n  undergrad_school = c(\"Harvard\", \"Cal Poly\", \"Berkeley\", \"Harvard\", \"Winona State University\"),\n  grad_school      = c(\"UNC\", \"Boston University\", \"UCLA\", \"Stanford\", \"University of Nebraska-Lincoln\")\n)\n\nprof_course &lt;- data.frame(\n  professor = c(\"Bodwin\", \"Glanz\", \"Carlton\", \"Theobold\", \"Robinson\"),\n  Stat_331  = c(TRUE, TRUE, TRUE, TRUE, TRUE),\n  Stat_330  = c(FALSE, TRUE, TRUE, FALSE, TRUE),\n  Stat_431  = c(TRUE, TRUE, FALSE, TRUE, FALSE)\n)\n\ncourse_info &lt;- data.frame(\n  course       = c(\"Stat_331\", \"Stat_330\", \"Stat_431\"),\n  num_sections = c(8, 3, 1)\n)\n\nHere is what they look like once created:\n\nprof_info\n\n  professor        undergrad_school                    grad_school\n1    Bodwin                 Harvard                            UNC\n2     Glanz                Cal Poly              Boston University\n3   Carlton                Berkeley                           UCLA\n4       Sun                 Harvard                       Stanford\n5  Robinson Winona State University University of Nebraska-Lincoln\n\n\n\nprof_course\n\n  professor Stat_331 Stat_330 Stat_431\n1    Bodwin     TRUE    FALSE     TRUE\n2     Glanz     TRUE     TRUE     TRUE\n3   Carlton     TRUE     TRUE    FALSE\n4  Theobold     TRUE    FALSE     TRUE\n5  Robinson     TRUE     TRUE    FALSE\n\n\n\ncourse_info\n\n    course num_sections\n1 Stat_331            8\n2 Stat_330            3\n3 Stat_431            1\n\n\nThese data sets contain information about five Cal Poly professors, their educational history, the classes they are able to teach, and the number of sections of each class that need to be assigned.\n\n\nQuestion 2 Combine data sets prof_info and prof_course to make this data set:\n\n\n\n\n\nprofessor\nundergrad_school\ngrad_school\nStat_331\nStat_330\nStat_431\n\n\n\nBodwin\nHarvard\nUNC\nTRUE\nFALSE\nTRUE\n\n\nGlanz\nCal Poly\nBoston University\nTRUE\nTRUE\nTRUE\n\n\nCarlton\nBerkeley\nUCLA\nTRUE\nTRUE\nFALSE\n\n\nRobinson\nWinona State University\nUniversity of Nebraska-Lincoln\nTRUE\nTRUE\nFALSE\n\n\n\n\n\n\n\nQuestion 3 Combine data sets prof_info and prof_course to make this data set:\n\n\n\n\n\nprofessor\nundergrad_school\ngrad_school\nStat_331\nStat_330\nStat_431\n\n\n\nBodwin\nHarvard\nUNC\nTRUE\nFALSE\nTRUE\n\n\nGlanz\nCal Poly\nBoston University\nTRUE\nTRUE\nTRUE\n\n\nCarlton\nBerkeley\nUCLA\nTRUE\nTRUE\nFALSE\n\n\nSun\nHarvard\nStanford\nNA\nNA\nNA\n\n\nRobinson\nWinona State University\nUniversity of Nebraska-Lincoln\nTRUE\nTRUE\nFALSE\n\n\n\n\n\n\n\nQuestion 4 Transform and combine data sets prof_course and course_info to make this data set:\n\n\n\n\n\nprofessor\ncourse\ncan_teach\nnum_sections\n\n\n\nBodwin\nStat_331\nTRUE\n8\n\n\nBodwin\nStat_330\nFALSE\n3\n\n\nBodwin\nStat_431\nTRUE\n1\n\n\nGlanz\nStat_331\nTRUE\n8\n\n\nGlanz\nStat_330\nTRUE\n3\n\n\nGlanz\nStat_431\nTRUE\n1\n\n\nCarlton\nStat_331\nTRUE\n8\n\n\nCarlton\nStat_330\nTRUE\n3\n\n\nCarlton\nStat_431\nFALSE\n1\n\n\nTheobold\nStat_331\nTRUE\n8\n\n\nTheobold\nStat_330\nFALSE\n3\n\n\nTheobold\nStat_431\nTRUE\n1\n\n\nRobinson\nStat_331\nTRUE\n8\n\n\nRobinson\nStat_330\nTRUE\n3\n\n\nRobinson\nStat_431\nFALSE\n1"
  },
  {
    "objectID": "04-data-joins-and-transformations.html#pa-4.1-government-spending",
    "href": "04-data-joins-and-transformations.html#pa-4.1-government-spending",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "PA 4.1: Government Spending",
    "text": "PA 4.1: Government Spending\nThis week you will be tidying untidy data to explore the relationship between countries of the world and military spending. You will then be using this cleaned data set to create a visualization by using properties of factors to enhance the order of your plots.\nVisit PA 4: Government Spending for instructions.\nSubmit your answer to the Canvas Quiz."
  }
]